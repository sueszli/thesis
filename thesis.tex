% - submission dates: https://informatics.tuwien.ac.at/study-services/dates/
% - submission steps: https://informatics.tuwien.ac.at/study-services/master-graduation/
% - thesis examples: https://catalogplus.tuwien.at/primo-explore/search?query=any,exact,E%20066%20645,AND&pfilter=dr_s,exact,20180101,AND&pfilter=dr_e,exact,20251231,AND&tab=default_tab&search_scope=UTW&sortby=rank&vid=UTW&mode=advanced&offset=0

\documentclass[a4paper, oneside]{discothesis}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{epstopdf}
\usepackage{epigraph}
\usepackage{amsmath}
\usepackage{minted}
\usepackage{anyfontsize}
\usepackage{hyperref}
\usepackage{mathtools}
\usepackage[many]{tcolorbox}
\usepackage{mathspec}
\usepackage{setspace}
\usepackage{multicol}
\usepackage{multirow}
\usepackage{cleveref}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{booktabs}
\usepackage{scrlayer-scrpage}
\usepackage{setspace}
\usepackage{cite}

%
% customization
%

\usepackage[margin=0pt, top=30mm, bottom=20mm, left=20mm, right=20mm]{geometry} % drop margin
\setlength\parindent{0pt} % drop indent

% drop page numbers at footer
\makeatletter
\let\@oddfoot\@empty
\let\@evenfoot\@empty
\makeatother
\cfoot[]{}

% make subcaption tiny for 2x8 grid
\captionsetup[subfigure]{labelsep=none}
\subcaptionsetup{font=normalsize}

% make urls small, drop underline
\hypersetup{colorlinks = true, linkcolor = black, urlcolor = black, citecolor = black,}
\urlstyle{same}
\makeatletter
\def\url@leostyle{%
  \@ifundefined{selectfont}{\def\UrlFont{\sf}}{\def\UrlFont{\small\ttfamily}}}
\makeatother  
\urlstyle{leo}

% custom shortcuts
\newcommand{\linebreaks}{\vspace*{0.5em}}

% highlight boxes
% - https://www.overleaf.com/latex/examples/simple-stylish-box-design/stzmmcshxdng
% - https://tex.stackexchange.com/questions/66820/how-to-create-highlight-boxes-in-latex
% - https://tex.stackexchange.com/questions/141569/highlight-textcolor-and-boldface-simultaneously
\tcbset{sharp corners, colback = white, before skip = 0.5cm, after skip = 0.5cm, breakable}
\newtcolorbox{highlightbox}{sharpish corners, boxrule = 0pt, leftrule = 4.5pt, enhanced, breakable, fuzzy shadow = {0pt}{-2pt}{-0.5pt}{0.5pt}{black!35}}
\newtcolorbox{quotebox}{sharpish corners, boxrule = 0pt, rightrule = 4.5pt, enhanced, breakable, fuzzy shadow = {0pt}{-2pt}{-0.5pt}{0.5pt}{black!35}}

% 
% header
% 

\thesistype{Master's Thesis}
\title{Rethinking Adversarial Examples}

\author{Yahya Jabary}
\email{yjabary@ethz.ch}

\institute{Computer Engineering and Networks Laboratory \\[2pt] ETH Zürich}

% \logo{\includegraphics[width=0.2\columnwidth]{figures/dsg-logo}}

\supervisors{Andreas Plesner\\[2pt] Prof.\ Dr.\ Roger Wattenhofer\\[8pt] Alireza Furutanpey\\[2pt] Prof.\ Dr.\ Schahram Dustdar}

\keywords{Reliability, Robustness, Security, Algorithmic Models}
% \categories{ACM categories go here.}

\date{\today}

\begin{document}

\frontmatter
\maketitle

\cleardoublepage

\begin{acknowledgements}
	\begin{quotebox}
		\begin{flushright}
			\textit{``Well, at a high level, I think that the goal of computer security is to ensure that systems do the right thing, even in the presence of malicious inputs. Now, achieving this goal in the context of machine learning is exceptionally challenging for two reasons. [...] So first of all, computer scientists lack a deep mathematical understanding of how machine learning actually learns and predicts. [...] And second, many people who deploy machine learning don't actually care about the first problem.''}
			\vspace{1em}
	
			\textit{``And if somebody asks you why the stuff worked, you just say the stuff is what the stuff is brother, accept the mystery. Okay. And so basically machine learning is like this, right? So we've invented a bunch of techniques that kind of work, like in some cases, but we're not really sure what's going on. [...] We don't ask questions about the weights. We just wake up, we go to work, we use the weights, we go back home. Okay. If we change the weights, the predictions would be different and less good, probably... depending on the weather... so we don't ask about the weights.''}
			\vspace{1em}
	
			James Mickens, USENIX'18~\cite{218395}
		\end{flushright}
	\end{quotebox}

	\linebreaks

	The most rewarding part of this project was working on a problem that truly matters to me, alongside people who genuinely care. For the first time, I felt a sense of belonging.

	I'm deeply grateful to those who supported me along the way. My parents, Shima and Florian and my family, for their unconditional support \textendash{} even when I quit my job to pursue my passion. My partner, Laura, whose love and encouragement crossed the Atlantic and carried me through many long nights.

	I owe much to those who made this work possible. Prof.\ Roger Wattenhofer, for trusting me with this project and guiding me with wisdom and humor. Andreas Plesner, who was just as much of a mentor as a collaborator, for his dedication to our vision. Turlan Kuzhagaliyev, for keeping me grounded and focused.

	I also value the friendships I made throughout this journey. Prof.\ Nils Lukas, who first introduced me to ML-Security and was always there to discuss ideas. Alireza Furutanpey, for his camaraderie and sharing his boundless passion.

	Thanks as well to those whose paths have diverged from mine but whose impact remains with me, including Prof.\ Schahram Dustdar, who enabled me to study abroad.

	To me, adversarial examples are also a metaphor for having a strong character by being open-minded. They show how subtle differences in perspective can lead to vastly different interpretations and outcomes.
	
	I hope to continue this journey with the same spirit that brought me here.
\end{acknowledgements}

\begin{abstract}
	% short. state what you did and what the most important result is.
	Adversarial machine learning has traditionally focused on imperceptible perturbations that fool deep neural networks. This thesis challenges that narrow view by examining unrestricted adversarial examples \textendash{} a broader class of manipulations that can compromise model security while preserving semantics.

	Through extensive experiments, we make three key contributions: First, we demonstrate that the standard imperceptibility constraint is insufficient for characterizing real-world adversarial threats through a comprehensive survey of current research. Second, we develop a novel and computationally efficient method for generating adversarial examples using geometric masks inspired by hCAPTCHA challenges. Our approach creates adversarial examples that are (1) effective, (2) transferable between models and (3) traceable in the model's decision space \textendash{} achieving comparable misclassification rates to existing techniques while requiring significantly less compute. Finally, we investigate improving model robustness by creating ensembles from intermediary ResNet layers using linear probes, combined with nature-inspired noise during training. While this architectural approach shows promise, we find that achieving ``zero-cost robustness'' remains elusive without adversarial training.

	This work advances our understanding of adversarial examples beyond pixel-space perturbations and provides practical tools for both generating and defending against them.
	
	Our findings highlight the need to rethink how we conceptualize and evaluate adversarial robustness in machine learning systems.
\end{abstract}

\begin{zusammenfassung}
	% feindliche angriffe / adversarielle beispiele
	Das Maschinellelernwidrigkeitsforschungsgebietsuntersuchungswesen hat sich traditionellerweise auf Unmerklichkeitsveränderungsmanipulationsvorgehensweisen fokussieret, welche die Tiefeneuronalennetzwerkverarbeitungssysteme täuschen. Diese Dissertationsforschungsschriftendokumentation herausfordert selbige eingeschränkte Betrachtungsweisenmethodologie durch die Untersuchungsanalyse von uneingeschränkten Widersacherischenexemplifizierungsmanifestationen \textendash{} eine umfänglichere Klassifizierungskategorisierung von Manipulationsverfahrensweisenmethodologien, welche die Modellsicherheitsverhältnisgarantien kompromittieren können, während die Semantikbedeutungserhaltung bewahret wird.

	Durch umfängliche Experimentalforschungsuntersuchungsdurchführungen leisten wir drei hauptsächliche Beitragsleistungsmanifestationen: Erstens demonstrieren wir, dass die standardisierte Unmerklichkeitseinschränkungsbedingungsvoraussetzung unzureichend ist für die Charakterisierungsbestimmung realweltlicher widersacherischer Bedrohungsszenarienkonstellationen durch eine allumfassende Begutachtungsuntersuchung gegenwärtiger Forschungsarbeitsbestrebungen. Zweitens entwickeln wir eine neuartige und recheneffizienzoptimierte Methodologievorgehensweise zur Generierungsherstellung widersacherischer Exemplifizierungsmanifestationen unter Verwendung geometrischer Maskierungsvorrichtungsapparaturen, inspirieret durch hCAPTCHA-Herausforderungsaufgabenstellungen.
	
	Unsere Herangehensweisenmethodologie erschaffet widersacherische Exemplifizierungsmanifestationen, welche (1) wirkungsvollheitspotential, (2) zwischenmodellübertragungsfähigkeitscharakteristika und (3) modellentscheidungsraumsnachvollziehbarkeitsqualitäten aufweisen \textendash{} vergleichbare Fehlklassifizierungsratenleistungen zu existierenden Technikmethodologien erreichend bei wesentlich geringerem Berechnungsaufwandserfordernis.
	
	Schlussendlich investigieren wir die Verbesserungspotentialausschöpfung der Modellrobustheitseigenschaftscharakteristika durch die Kreierungsbestrebungen von Ensemblezusammenstellungskonfigurationen aus intermediären ResNet-Schichtenstrukturarchitekturen unter Verwendung linearer Sondierungsapparaturmechanismen, kombinieret mit naturinspiriertem Rauschverhaltensphänomen während des Trainierungsprozessablaufes.
	
	Diese Forschungsarbeitsbestrebung erweitert unser Verständnishorizontpotential widersacherischer Exemplifizierungsmanifestationen jenseits von Pixelraumperturbationsmodifikationen und stellet praktische Werkzeugunterstützungsmechanismen zur Verfügung. Unsere Erkenntnisgewinnungsresultate unterstreichen die Notwendigkeitserfordernis, unsere Konzeptualisierungsherangehensweise und Evaluierungsmethodologien der widersacherischen Robustheitseigenschaftscharakteristika in Maschinellenlernverarbeitungssystemen zu überdenken.

	% Das Maschinenlernwidrigkeitsforschungsgebiet hat sich traditionellerweise auf unmerkliche Veränderungsmanipulationen fokussieret, welche die tiefeneuronalen Netzwerke täuschen. Diese Dissertationsschrift challengieret selbige eingeschränkte Betrachtungsweise durch die Untersuchung von uneingeschränkten widersacherischen Exemplifizierungen \textendash{} eine umfänglichere Klassifizierung von Manipulationsverfahren, welche die Modellsicherheitsverhältnisse kompromittieren können, während die Semantik bewahret wird.

	% Durch umfangreiche Experimentalforschungsuntersuchungen leisten wir drei hauptsächliche Beitragsleistungen: Erstens demonstrieren wir, dass die standardisierte Unmerklichkeitseinschränkungsbedingung unzureichend ist für die Charakterisierung realweltlicher widersacherischer Bedrohungsszenarien durch eine allumfassende Begutachtung gegenwärtiger Forschungsarbeit. Zweitens entwickeln wir eine neuartige und recheneffiziente Methodologie zur Generierung widersacherischer Exemplifizierungen unter Verwendung geometrischer Maskierungsvorrichtungen, inspirieret durch hCAPTCHA-Herausforderungsaufgaben. Unsere Herangehensweise erschaffet widersacherische Exemplifizierungen, welche (1) wirkungsvoll, (2) zwischen Modellen übertragbar und (3) im Modellentscheidungsraume nachvollziehbar sind \textendash{} vergleichbare Fehlklassifizierungsraten zu existierenden Techniken erreichend bei signifikant geringerem Berechnungsaufwande.
	
	% Schlussendlich investigieren wir die Verbesserung der Modellrobustheitseigenschaften durch die Kreierung von Ensemblezusammenstellungen aus intermediären ResNet-Schichtenstrukturen unter Verwendung linearer Sondierungsapparaturen, kombinieret mit naturinspiriertem Rauschverhalten während des Trainierungsprozesses. Während dieser architektonische Ansatz vielversprechend erscheinet, stellen wir fest, dass das Erreichen einer ``kostenfreien Robustheitseigenschaft'' ohne widersacherisches Training schwer erreichbar bleibet.
	
	% Diese Arbeit erweitert unser Verständnis widersacherischer Exemplifizierungen jenseits von Pixelraumperturbationen und stellet praktische Werkzeuge sowohl für die Generierung als auch die Verteidigung gegen selbige zur Verfügung. Unsere Erkenntnisse unterstreichen die Notwendigkeit, unsere Konzeptualisierungs- und Evaluierungsweise der widersacherischen Robustheitseigenschaften in Maschinenlehrnsystemen zu überdenken.
\end{zusammenfassung}

\chapter*{Originality}

I hereby declare that I have written this thesis independently, that I have completely specified the utilized sources and resources and that I have definitely marked all parts of the work \textendash{} including tables, maps and figures \textendash{} which belong to other works or to the internet, literally or extracted, by referencing the source as borrowed.

I further declare that I have used generative AI tools only as an aid, and that my own intellectual and creative efforts predominate in this work. In the appendix ``Overview of Generative AI Tools Used'' I have listed all generative AI tools that were used in the creation of this work, and indicated where in the work they were used. If whole passages of text were used without substantial changes, I have indicated the input (prompts) I formulated and the IT application used with its product name and version number/date.

Yahya Jabary, \today. Signed digitally.

\section*{Papers}

% see: https://arxiv.org/pdf/2202.02435

\begin{spacing}{1}
	\textbf{Seeing Through the Mask: Rethinking Adversarial Examples for CAPTCHAs} \\
	Yahya Jabary andreas Plesner, Turlan Kuzhagaliyev, Roger Wattenhofer \\
	\textit{ArXiv: 2409.05558}
\end{spacing}

\section*{Open source software}

The majority of time working on this thesis was spent on developing a reproducible research pipeline for experiments in a compute and GPU memory constrained, containerized environment with locked dependencies.

Due to the exploratory nature of the work, many of the software built and experiments conducted had to be discarded.

The following projects were developed as part of this work (in chronological order, with the most recent first):

\linebreaks

\begin{samepage}
	\begin{spacing}{1}
		\textbf{self-ensembling} \\
		All experiments related to the Self-Ensembling algorithm by Fort et al. \\
		\url{https://github.com/ETH-DISCO/self-ensembling} \\
		\url{https://huggingface.co/sueszli/self-ensembling-resnet152}
	\end{spacing}
\end{samepage}

\linebreaks

\begin{samepage}
	\begin{spacing}{1}
		\textbf{ensemble-everything-everywhere} \\
		Pull Request: Optimizing the official Self-Ensembler repository by Fort et al. \\
		\url{https://github.com/stanislavfort/ensemble-everything-everywhere/pull/2}
	\end{spacing}
\end{samepage}

\linebreaks

\begin{samepage}
	\begin{spacing}{1}
		\textbf{vision} \\
		Pull Request: Containerizing TorchVision to build ResNet from scratch. \\
		\url{https://github.com/pytorch/vision/pull/8652}
	\end{spacing}
\end{samepage}

\linebreaks

\begin{samepage}
	\begin{spacing}{1}
		\textbf{advx-bench} \\
		All experiments related to the geometric masks from the paper. \\
		\url{https://github.com/ETH-DISCO/advx-bench} \\
		\url{https://huggingface.co/sueszli/robustified_clip_vit}
	\end{spacing}
\end{samepage}

\linebreaks

\begin{samepage}
	\begin{spacing}{1}
		\textbf{cluster-tutorial} \\
		Tutorial on how to circumvent the distributed NFS4 filesystem by attaching the terminal to an interactive SLURM job, running an Apptainer to provide root privileges and redirecting all file pointers to the EXT4 filesystem to avoid out-of-memory OS errors. Also runs a Jupyterlab instance on the intranet for ease of use. \\
		\url{https://github.com/ETH-DISCO/cluster-tutorial}
	\end{spacing}
\end{samepage}

\linebreaks

\begin{samepage}
	\begin{spacing}{1}
		\textbf{python-template} \\
		Short scripts to \texttt{pip-compile} dependencies, containerize the environment and translate back and forth between Conda and Docker for different job submission systems. Also a simple job watchdog for long-running processes. \\ 
		\url{https://github.com/sueszli/python-template/}
	\end{spacing}
\end{samepage}
	
\linebreaks

\begin{samepage}
	\begin{spacing}{1}
		\textbf{captcha-the-flag} \\
		Cybersecurity emulation for CAPTCHAs: A deployable replica of Google's reCAPTCHAv2 and a scraper used to evaluate challenges against solvers. \\
		\url{https://github.com/ETH-DISCO/captcha-the-flag}
	\end{spacing}
\end{samepage}

\section*{Breakdown of contributions}

For the paper Andreas Plesner had the original idea. The written text was joint work between all authors, with Prof.\ Roger Wattenhofer taking the lead on creating a cohesive narrative for our experiments. Andreas and I writing the majority of the text. Turlan and I conducting all experiments. The TU Wien DSG lab provided computational resources for robustifying a ResNet model, which we had to discard. Alireza Furutanpey suggested using LPIPS as a metric to evaluate the perceptual quality of adversarial examples, which we incorporated into our weighted objective function. Additionally, he helped with general advice on PyTorch.

Regarding the developed software, all contributions are my own, unless stated otherwise in the repository. A prototype of the self-ensembled ResNet model was developed by Andreas Plesner, but the authors soon released their own implementation, which was then used in all experiments for consistency.

Andreas Plesner and Maximilian Seeliger diligently proofread this manuscript for errors. As is traditional, any errors that remain, are of course, mine alone.

\tableofcontents

\mainmatter

\chapter{Introduction}

We have two goals in writing this document. One: fulfilling the requirements for a master's degree by presenting and extending our original research~\cite{jabary2024seeing} in thesis form. Two: offering a fresh and cohesive perspective on the rapidly evolving and, in our view, really exciting field of adversarial machine learning to a broader audience, with fewer technical prerequisites. We hope it will be valuable to those interested.

\section{Definition}

Adversarial examples are closely related to the concept of perturbation methods\footnote{Thanks to Prof.\ Roger Wattenhofer for sharing this piece of unorthodox history.}.

\subsection{Perturbation Methods}

The origin of perturbations can be traced back to the early days of computational geometry by Seidel et al. in 1998~\cite{seidel1998nature}. Perturbation techniques in computational geometry address a fundamental challenge: handling ``degeneracies'' in geometric algorithms. These are special cases that occur when geometric primitives align in ways that break the general position assumptions the algorithms rely on.

\begin{highlightbox}
	\textbf{Example:} Perturbation scheme for a Linear Classifier \\

	Consider a simple case of determining whether a point lies above or below a line~\cite{de2000computational}. While this classification appears straightforward, numerical issues arise when the point lies exactly on the line. Such degeneracies can cascade into algorithm failures or inconsistent results. The elegant solution is to imagine slightly moving (perturbing) the geometric objects to eliminate these special cases. Formally, we can express symbolic perturbation as $p_\varepsilon(x) = x + \varepsilon \cdot \delta(x)$ where $x$ is the original input, $\varepsilon$ is an infinitesimally small positive number, the exact value of which is unimportant and $\delta(x)$ is the perturbation function to break degeneracies. \\

	A perturbation scheme should be (1) consistent, meaning that the same input always produces the same perturbed output (2) infinitesimal, such that perturbations are small enough not to affect non-degenerate cases and (3) effective in breaking all possible degeneracies. \\

	One powerful perturbation approach is Simulation of Simplicity (SoS)~\cite{franklin2022implementing, edelsbrunner2002topological, edelsbrunner2001sink, edelsbrunner1990simulation, levy2016robustness, schorn1993axiomatic}. SoS systematically perturbs input coordinates using powers of a symbolic infinitesimal. For a point $p_i = (x_i, y_i)$, the perturbed coordinates become:

	$$(\tilde{x_i}, \tilde{y_i}) = (x_i + \varepsilon^{2i}, y_i + \varepsilon^{2i+1}) = p_i + \varepsilon^{2i} \cdot (1, \varepsilon)$$

	This scheme ensures that no two perturbed points share any coordinate, effectively eliminating collinearity and other degeneracies.
\end{highlightbox}

The beauty of perturbation methods lies in their ability to handle degeneracies without explicitly detecting them, making geometric algorithms both simpler and more robust.

\subsection{Imperceptible Adversarial Examples}

Adversarial examples, first introduced by Szegedy et al.\ in 2014~\cite{szegedy2013intriguing}, follow the same principles as perturbation methods, but with the opposite objective. Instead of seeking to eliminate degeneracies (brittleness in the decision boundary), they exploit them to cause targeted misclassifications. Intuitively, they can be understood as seeking the closest point in the input space that lies on the ``wrong side'' of a decision boundary relative to the original input. This shift, applied to the original input, creates an adversarial example.

\begin{highlightbox}
	\textbf{Example:} Fast Gradient Sign Method (FGSM) \\

	FGSM is one of the earliest and most widely recognized adversarial attack techniques, introduced by Goodfellow et al.~\cite{goodfellow2014explaining} in the context of visual recognition tasks. Given an input image $x$, FGSM generates an adversarial example $x'$ by perturbing the input in the direction of the gradient of the loss function with respect to the input.\\

	The perturbation is controlled by a parameter $\varepsilon > 0$~\footnote{Commonly $\varepsilon = 8/255$ for 8-bit images, so it stays within the precision constraints of the pixel values.}, which determines the magnitude of the change based on the direction of change for each pixel or feature in the input $x$. The model's loss function denoted by $J$, $\theta$ represents the model's parameters and $y$ is the true target label. \\
	
	It works by calculating the gradient of the loss function with respect to the input, $\nabla_x J(\theta, x, y)$ and then adjusting the input in the direction of this gradient. The sign of the gradient, $\text{sign}(\nabla_x J(\theta, x, y))$, is used to ensure that the perturbation is small, while the $\ell_\infty$-norm constraint ensures that the change to the input remains ``imperceptible'' to human observers~\cite{goodfellow2014explaining, zhang2019adversarial}. More on the concept of imperceptibility later.\\
	
	The process for generating an adversarial example with FGSM can be expressed as:
	
	$$x' = x + \underbracket{\varepsilon \cdot \text{sign}(\nabla_x J(\theta, x, y))}_{\text{Perturbation}}$$
	
	In the untargeted version, the perturbation is designed to increase the loss for the correct class. In the targeted version the perturbation is designed to minimize the loss with respect to the adversary's chosen target class, making the model predict it deliberately.
\end{highlightbox}

\begin{highlightbox}
	\textbf{Digression:} Pixel-space constraints do not guarantee imperceptibility \\

	Traditionally, adversarial examples are expected to have two key properties: (1) they should successfully cause misclassification in targeted models while (2) remaining imperceptible to human observers~\cite{cubuk2017intriguing}. \\

	However, the concept of ``imperceptibility [to humans]'' as originally proposed by Szegedy et al.~\cite{szegedy2013intriguing} by limiting pixel-space perturbations through an $\varepsilon$-bounded constraint is fundamentally flawed. This is because the human visual system is not solely reliant on pixel-space information to interpret images~\cite{mentzer2020high, ning2023hflic}. \\

	Humans can detect forged low-$\varepsilon$ adversarial examples with high accuracy in both the visual (85.4\%)~\cite{veerabadran2023subtle} and textual ($\geq$70\%)~\cite{herel2023preserving} domain. It's worth mentioning that invertible neural networks can partially mitigate this issue in the visual domain~\cite{chen2023imperceptible}. \\
	
	% see: https://gwern.net/doc/ai/nn/adversarial/human/index#harrington-deza-2021-section
	Additionally, small $\varepsilon$-bounded adversarial perturbations are found to cause misclassification in time-constrained humans~\cite{elsayed2018adversarial} and primates~\cite{yuan2020fooling}.
\end{highlightbox}

\begin{highlightbox}
	\textbf{Intuition:} The Deep Learning Hypothesis \\

	Ilya Sutskever, in his ``Test of Time'' award talk at NeurIPS 2024~\cite{sutskever2014sequence}, revisited an idea he had previously only hinted at in interviews. This heuristic has since gained widespread acceptance and is now even taught in introductory machine learning courses~\cite{guerzhoy_ann}. \\

	The idea is straightforward: Human perception operates at a rapid pace. Neurons in the human brain can fire up to 100 times per second. Humans can complete simple perceptual tasks within 0.1 seconds. This implies that neurons fire in a sequence of at most 10 times for such tasks. Consequently, any task that a human can perform in 0.1 seconds can also be accomplished by a deep neural network with approximately 10 layers~\cite{sutskever2014sequence}. \\

	This could explain why adversarial examples transfer to time-constrained humans~\cite{elsayed2018adversarial}. It also suggests that there may be a fundamental limit to the robustness of deep learning models, as they are inherently limited by the speed of their computations. \\

	At first glance this idea might seem contradictory to the universal approximation theorem (UAT). However, the UAT only guarantees the existence of a network that can approximate any continuous function, not the efficiency or speed of computation~\cite{hornik1989multilayer}.
\end{highlightbox}

While initially discovered in computer vision applications, the attack can be crafted for any domain or data type, even graphs~\cite{Kashyap2024AdversarialAA}. Natural language processing models can be attacked by circumventing the discrete nature of text data~\cite{Han2022TextAA, meng2020geometry, yang2024assessing}. Speech recognition systems are vulnerable to audio-based attacks, where crafted noise can cause system failure~\cite{rajaratnam2018noise}. Deep reinforcement learning applications, including pathfinding and robot control, have also shown susceptibility to adversarial manipulations that can compromise their decision-making capabilities~\cite{Bai2018AdversarialEC}.

\subsection{Semantics Preserving Adversarial Examples}

Imperceptible noise-based adversarial examples are just one type of semantics-preserving adversarial examples. Other examples include rotating an image by a few degrees or capturing it from a different angle, which can also cause misclassification. These broader categories of adversarial examples are often referred to as ``unrestricted''~\cite{fazlija2024real, brown2018unrestricted} or ``semantics-preserving''~\cite{browne2020semantics, careil2023towards, lee2020semantics}. The comparison in Fig.~\ref{fig:unrestricted} and the illustration in Fig.~\ref{fig:latentspace} highlight the differences between various kinds of adversarial examples. Fig.~\ref{fig:adversarial_dogs} shows a collection of naturally occurring adversarial examples, also known as ``natural adversarial examples''~\cite{hendrycks2021natural, teenybiscuittweet}.

This shift in defining adversarial examples, popularized by the ``Unrestricted Adversarial Examples Challenge''~\cite{brown2018unrestricted} by Google in 2018, has led to a more nuanced understanding of the phenomenon. It acknowledges that real-world applications, especially in safety-critical contexts, are subject to a broader range of adversarial attacks than previously assumed and do not always adhere to the ``small perturbation'' constraint initially proposed~\cite{brown2018unrestricted}.

This paradigm shift towards seeking more meaningful adversarial examples and ``spatial robustness'' was first proposed by Gilmer et al.\ in 2018~\cite{gilmer2018motivating} and further explored by Engstrom et al.\ in 2019~\cite{engstrom2019exploring}. These works lay the theoretical foundation for our research and we believe this approach to be the most promising for future research in adversarial machine learning.

The challenge of defining semantics is central to this discussion. Without perfect representations that align with human judgment functions, we must rely on the best available encoders or semantics preservation metrics~\cite{engstrom2019exploring, herel2023preserving} as proxies. This pragmatic approach acknowledges the limitations of current technology while striving for more meaningful adversarial examples.

\begin{figure}
	\centering
	\includegraphics[width=0.45\columnwidth]{figures/unrestricted-advx.png}
	\caption{Unrestricted adversarial examples~\cite{brown2018unrestricted}.}
	\label{fig:unrestricted}
\end{figure}

\begin{figure}
	\centering
	\includegraphics[width=0.75\columnwidth]{figures/latentspace.pdf}
	\caption{Semantics preserving/changing perturbations in pixel/latent-space (assuming full accuracy).}
	\label{fig:latentspace}
\end{figure}

\begin{figure}[th]
	\centering
	\begin{minipage}[t]{0.24\textwidth}\centering\includegraphics[width=\textwidth]{figures/dog-vs-bagel.jpeg}\end{minipage}
	\hfill
	\begin{minipage}[t]{0.24\textwidth}\centering\includegraphics[width=\textwidth]{figures/dog-vs-mop.jpeg}\end{minipage}
	\hfill
	\begin{minipage}[t]{0.24\textwidth}\centering\includegraphics[width=\textwidth]{figures/dog-vs-chicken.jpeg}\end{minipage}
	\hfill
	\begin{minipage}[t]{0.24\textwidth}\centering\includegraphics[width=\textwidth]{figures/dog-vs-muffin.jpeg}\end{minipage}
	
	\vspace{1em}

	\begin{minipage}[t]{0.24\textwidth}\centering\includegraphics[width=\textwidth]{figures/dog-vs-loaf.jpeg}\end{minipage}
	\hfill
	\begin{minipage}[t]{0.24\textwidth}\centering\includegraphics[width=\textwidth]{figures/dog-vs-crossaint.jpeg}\end{minipage}
	\hfill
	\begin{minipage}[t]{0.24\textwidth}\centering\includegraphics[width=\textwidth]{figures/dog-vs-bear.jpeg}\end{minipage}
	\hfill
	\begin{minipage}[t]{0.24\textwidth}\centering\includegraphics[width=\textwidth]{figures/dog-vs-towel.jpeg}\end{minipage}
	\caption{Natural adversarial examples~\cite{teenybiscuittweet}: Dog vs. Similar looking objects.}
	\label{fig:adversarial_dogs}
\end{figure}

\section{Motivation}

Machine learning systems are growing rapidly in scale, capability\footnote{Today's LLMs are no longer mere ``stochastic parrots'', as they demonstrate compositional generalization~\cite{qiu2024can, keysers2019measuring, Ananthaswamy2024}.} and are increasingly being deployed in critical applications~\cite{10585001, ifci2023AnalysisOT, 9099439, Khadka2022ResilientML, yilmaz2021privacy, apruzzese2023real, kumar2020legal, Cao2020HateGANAG, Nurseitov2022ApplicationOM, Zolotukhin2022AttacksAM, huggingface2024security}.

Ensuring the safety of these systems is widely recognized as one of the most impactful fields for addressing global challenges~\cite{80000hours_infosec, FLI2023pause, hendrycks2021unsolved}.

\paragraph{Neglectedness.}

The field of ML safety \textendash{} which encompasses research areas such as robustness (resilience to hazards), monitoring (hazard detection), alignment (guiding ML systems' behavior) and systemic safety (minimizing deployment risks) \textendash{} remains significantly overlooked compared to other domains of machine learning~\cite{hendrycks2021unsolved, 80000hours_infosec}. As estimated by Hilton et al.\ only about 0.1\% of the resources dedicated to advancing AI capabilities in 2021 were allocated toward mitigating AI risks~\cite{80000hours_ai_2024}. That is, despite a growing consensus that the risks posed by AI systems are significant and warrant urgent attention~\cite{FLI2023pause, hendrycks2021unsolved} and a near-exponential surge in academic interest in adversarial machine learning since 2014~\cite{carlini2019adversarial}.

\paragraph{Impact.}

Adversarial attacks and machine learning security extend beyond academic curiosity. Algorithmic models like deep neural networks face several challenges when deployed in high-stakes scenarios. (1) They fail to provide clear explanations for decisions, which makes their outcomes hard to trust. (2) They create additional security risks by expanding the potential attack surface, which we do not yet fully know how to defend. (3) They can be weaponized to uncover unknown vulnerabilities (zero-days) at an unprecedented scale.

The issue with a lack of interpretability in high-stakes decisions and model vulnerability appears clearly in areas like autonomous weapons~\cite{Miller2017ComputerVO}, critical national infrastructure~\cite{Moradpoor2023TheTO, Chevardin2023AnalysisOA, Ulybyshev2021TrustworthyDA, Halak2022TowardsAP, Rudolph2008DevelopingPS}, financial fraud detection~\cite{Gu2022DeepLT, Agarwal2021BlackBoxAE, Tsai2024EffectiveAE}, healthcare diagnostics~\cite{jogani2022analysis, najafi2024dft, 9154468}, autonomous vehicles~\cite{Patel2019AdaptiveAV, Ji2021PoltergeistAA, Axelrod2017CybersecurityCO} and cybersecurity~\cite{Yuan2023MultiSpacePhishET, to2023effectiveness, eisenhofer2023no}. Another, more day-to-day example of a model's vulnerability is the exploitation of conference paper-reviewer assignment systems, enabling the adversary to preselect reviewers to gain a competitive advantage~\cite{eisenhofer2023no}.

Finally, tools such as the VulnHuntr~\cite{vulnhuntr} and Big Sleep~\cite{bigsleep2024} models have high potential to be misused for malicious purposes. The latter has recently been particularly successful in automatically detecting zero-day exploits in SQLite~\cite{bigsleep2024}.

This has lead to major companies investing heavily in adversarial machine learning research and security.

\paragraph{Funding.}

Microsoft has taken a leading position, spending over \$20 billion on cybersecurity initiatives, with a significant portion dedicated to machine learning security research and their specialized ML red team operations~\cite{coursera_adversarial_2024}.

Open Philanthropy has provided \$330,000 and \$343,235 in funding to Carnegie Mellon University dedicated to adversarial machine learning research~\cite{openphil2024adversarial}.

The MITRE corporation is now cooperating with Microsoft, Bosch, IBM, NVIDIA, Airbus, Deep Instinct and PricewaterhouseCoopers to develop the Adversarial Machine Learning Threat Matrix for threat modeling and risk assessment~\cite{mitre2024ml}.

The Defense Advanced Research Projects Agency (DARPA) has granted nearly \$1 million to the computer vision and adversarial machine learning research team at UC Riverside~\cite{roy2020darpa}.
Booz Allen Hamilton, the largest provider of machine learning services for the Federal government, invested in HiddenLayer, Robust Intelligence~\cite{robustintelligence2024, cai2020robust} Shift5, Credo, Hidden Level, Latent, Synthetaic and Reveal Technology~\cite{boozallen2023adversarial, boozallen2023adversarialother}, all of which are dedicated to machine learning security and robustness research.

These investments reflect a growing recognition of the importance of adversarial machine learning research and the need for robust, secure and reliable machine learning systems in the industry.

\section{Threat Modeling}

Having established the general concept of adversarial examples, we can now explore the various ways they can be categorized. Our system is not exhaustive: The field continues to evolve, with new attack vectors emerging regularly~\cite{Khaleel2024AdversarialAI}. This is particularly important in threat modeling, where the goal is to anticipate and defend against potential attacks.

We can differentiate between white-box and black-box attacks. White-box attacks assume complete knowledge of and access to the target model, while black-box attacks operate with limited or no access to the model's internal workings~\cite{capozzi2024adversarial}. Interestingly, research has shown that in some cases, black-box attacks can be more effective than white-box approaches at compromising model security~\cite{capozzi2024adversarial}.

An attack can be targeted or untargeted. Targeted attacks aim to manipulate the model into producing a specific, predetermined output, whereas untargeted attacks simply seek to cause any misclassification or erroneous output~\cite{capozzi2024adversarial, Kashyap2024AdversarialAA}. This distinction is particularly relevant in security-critical applications, where the attacker's goals may vary from causing general disruption to achieving specific malicious outcomes.

The method used to generate adversarial examples can be gradient-based, optimization-based or search-based strategies. For example, some text-based attacks leverage language models to generate alternatives for masked tokens, ensuring grammatical correctness and semantic coherence~\cite{garg2020bae}.

The extent to which adversarial examples are transferable \textendash{} meaning their ability to fool multiple different models or the human vision system\cite{elsayed2018adversarial} \textendash{} is another way to differentiate them. Some adversarial examples demonstrate high transferability across various model architectures, while others are more model-specific in their effectiveness~\cite{Li2022ASO, li2022review}. Recent research has shown that adversarial examples are more readily transferable between vanilla neural networks than between defended ones~\cite{li2019nattack, zheng2023black}.

Finally, attacks can either focus on preserving the semantic meaning of inputs or exploit the mathematical properties of models without regard for semantic interpretation~\cite{browne2020semantics}.

\section{Latent Representations}

The internal latent representations of neural networks, their alignment with human understanding and the resulting gap between the two (the human-machine vision gap~\cite{geirhos2021partial}) is a central theme in adversarial machine learning research. This gap has many practical implications for the robustness and interpretability of machine learning models.

Neural networks trained with topological features develop substantially different internal representations compared to those trained on raw data, though these differences can sometimes be reconciled through simple affine transformations~\cite{mcguire2023neural}. This finding suggests that while the structural representations may differ, the underlying semantic understanding might be preserved across different training approaches.

The Centered Kernel Alignment (CKA) metric enables us to compare neural network representations, though it comes with important caveats. In biological and artificial neural networks, CKA can show artificially high similarity scores in low-data, high-dimensionality scenarios, even with random matrices~\cite{murphy2024correcting}. This limitation is particularly relevant when comparing representations of different sizes or when analyzing specific regions of interest.

The relationship between network architecture and concept representation has also been explored. Generally higher-level concepts are typically better represented in the final layers of neural networks, while lower-level concepts are often better captured in middle layers~\cite{Agafonov2022AnEO, Agafonov2022LocalizationOO}. This hierarchical organization mirrors our understanding of human cognitive processing and suggests that neural networks naturally develop structured representations that align with human conceptual understanding.

The choice of an objective function significantly influences how networks represent information, particularly when dealing with biased data. Networks trained with Negative Log Likelihood and Softmax Cross-Entropy loss functions demonstrate comparable capabilities in developing robust representations~\cite{bangaru2022interpreting}.

Recent research~\cite{bansal2021revisiting} has demonstrated that neural networks with strong performance tend to learn similar internal representations, regardless of their training methodology. Networks trained through different approaches, such as supervised or self-supervised learning, can be effectively ``stitched'' together without significant performance degradation. This suggests a convergence in how successful neural networks represent information.

This aligns with the ``Platonic Representation Hypothesis'', which suggests that neural networks are converging toward a shared statistical model of reality, regardless of their training objectives or architectures~\cite{huh2024platonic}. As models become larger and are trained on more diverse tasks and data, their internal representations increasingly align with each other, even across different modalities like vision and language. This convergence appears to be driven by the fundamental constraints\footnote{Formally: ``If an optimal representation exists in function space, larger hypothesis spaces are more likely to cover it.''} of modeling the underlying structure of the real world, similar to Plato's concept of an ideal reality that exists beyond our sensory perceptions. The hypothesis proposes that this convergence is not coincidental but rather a natural consequence of different models attempting to capture the same underlying statistical patterns and relationships that exist in reality~\cite{huh2024platonic}. 

Should the ``Platonic Representation Hypothesis'' hold true, this would either mean that (a) adversarial examples as we know them are misalignments from a converged model of reality or (b) that there exist a universal adversarial example that can fool any model, regardless of its architecture, training data or objective function, converging to a single and shared model of reality.

Recent work by Moosavi-Dezfooli et al.\ \cite{moosavi2017universal} have demonstrated the existence of a single perturbation that can fool most models for all naturally occurring images, adding weight to the latter interpretation, though the question remains open.

\section{Mental Models}

The question discussed in the previous section is just one of many that remain open and yet have to be fully explained~\cite{shamir2021dimpled}. Among them are:

\begin{itemize}
	\setlength\itemsep{0em}
	\item What are adversarial examples?
	\item Why are the adversarial examples so close to the original images?
	\item Why do the adversarial perturbations not resemble the target class?
	\item Why do robustness and accuracy trade-off~\cite{zhang2019theoretically}?
	\item Why do adversarial examples transfer between models, even on disjoint training sets~\cite{szegedy2013intriguing}?
	\item Why do adversarial examples transfer between models~\cite{szegedy2013intriguing}?
	\item Why do adversarial examples transfer between models and time-limited humans~\cite{elsayed2018adversarial}?
\end{itemize}

Initially, when Szegedy et al.\ \cite{szegedy2013intriguing} coined the term they proposed that adversarial examples are caused by (1) neural networks developing internal representations that become increasingly disconnected from the input features as they progress through deeper layers and (2) that these networks fail to maintain the smoothness properties typically assumed in traditional machine learning approaches. The idea was that this lack of smoothness gives them their expressive power, but also makes them vulnerable to these attacks.

\begin{highlightbox}
	\textbf{Definition:} Manifold \\
	
	The first attempt to explain adversarial examples by Szegedy et al.\ \cite{szegedy2013intriguing} used the term ``manifold'', while referring to a data submanifold. \\
		
	A manifold can be thought of as a low-dimensional structure embedded in a high-dimensional space, representing the set of valid data points (e.g., natural images) that the neural network is trained to classify. Mathematically, if the input data lies on a manifold $\mathcal{M} \subset \mathbb{R}^m$, then $\mathcal{M}$ represents the subset of the high-dimensional input space $\mathbb{R}^m$ that corresponds to meaningful or real-world data. \\

	Szegedy et al.\ suggest that adversarial examples exploit the structure of this manifold and its surrounding space. Specifically, adversarial examples are small perturbations $r$ added to an input $x \in \mathcal{M}$, such that the perturbed input $x' = x + r$ lies off the data manifold but still within the high-dimensional input space. \\
	
	Formally, given a classifier $f: \mathbb{R}^m \to \{1, ..., k\}$ and its associated loss function $\text{Loss}_f(x, y)$, an adversarial example $x'$ for an input $x$ with true label $y$ can be found by solving:
	
	$$\min_{r} \|r\|_2 \quad \text{subject to } f(x + r) \neq y, \; x + r \in [0, 1]^m$$
	
	where $r$ is constrained to be small (e.g., in terms of its $L_2$-norm). This optimization problem effectively traverses the space near $x$, moving off the manifold $\mathcal{M}$, to find regions where the classifier's decision boundary behaves unexpectedly. \\
	
	The paper suggests that these adversarial examples expose ``blind spots'' in the learned representation of the manifold by the neural network. The network's decision boundary may extend into regions near $\mathcal{M}$ in ways that are not semantically meaningful, allowing adversarial perturbations to exploit these regions. This phenomenon arises due to the high dimensionality of the input space and the discontinuous mappings learned by deep networks, which can fail to generalize smoothly beyond the manifold~\cite{khoury2018geometry, Jha2018DetectingAE, Sha2020ADA, dube2018high, shamir2021dimpled}. \\
	
	A more rigorous definition of the manifold hypothesis is provided by Khoury et al.\ \cite{khoury2018geometry}.
\end{highlightbox}

\begin{highlightbox}
	\textbf{Definition:} Realism  \\

	A ``realistic subspace'' can be understood as a subset of the data manifold where the images appear plausible according to human perception or a given distribution $P$. A simple formula that expresses this idea elegantly to quantify realism is derived from the notion of randomness deficiency in algorithmic information theory~\cite{theis2024makes}:

	$$U(x) = -\log P(x) - K(x)$$

	where $P(x)$ is the probability density of the image $x$ under the target distribution and $K(x)$ is the Kolmogorov complexity of $x$, representing the shortest description of $x$ in a universal programming language. This measure, called a ``universal critic'', captures how well $x$ aligns with both the statistical properties of $P$ and its compressibility. A low value of $U(x)$ indicates that $x$ is realistic, while a high value suggests it is unrealistic~\cite{theis2024makes}. \\

	This approach generalizes prior methods by integrating both probabilistic and structural aspects of realism. It highlights that realism depends not only on adherence to statistical patterns (e.g., probabilities or divergences) but also on whether an image can be plausibly generated within the constraints of $P$. While directly computing $K(x)$ is infeasible due to its uncomputability, practical approximations (e.g., compression algorithms or neural network-based critics) can serve as proxies~\cite{theis2024makes}. \\

	The distinction between realistic and unrealistic perturbations is crucial for practical applications, as some adversarial examples may be mathematically valid but physically impossible to realize in real-world scenarios~\cite{dyrmishi2023empirical}. \\
	
	The challenge of quantifying realism remains a fundamental problem in machine learning~\cite{theis2024makes}.
\end{highlightbox}

Since then there have been many attempts at finding a cohesive narrative to explain these counter-intuitive properties, each with their own limitations and assumptions \textendash{} some complementary, some contradictory~\cite{ilyas2019adversarial}.

\paragraph{Non-robust features \& concentration of measure in high dimensions.} Most popularly, Ilyas et al.\ \cite{ilyas2019adversarial} proposed that features that models learn from can be divided in 3 categories: (1) useless featurs, to be discarded by the feature extractor, (2) robust features, which are comprehensible to humans, generalize across multiple datasets and remain stable under small adversarial perturbations and (3) non-robust features, which are incomprehensible to humans, learned by the supervised model to exploit patterns in the data distribution which are highly effective for the task at hand but also brittle and easily manipulated by adversarial perturbations. The authors suggest that the vulnerability of deep neural networks to adversarial examples is due to their reliance on non-robust features and inherent to how the models are optimized to minimize the loss function. In essence, the authors argue that adversarial vulnerability is a property of the dataset, not the algorithm and by removing these non-robust features from the training data although the adversarial robustness of the model can be improved, due to information loss of the most predictive features, the model's overall accuracy will decrease. This view is also shared among~\cite{engstrom2019a, raghunathan2018certified, wong2018provable, xiao2018training, cohen2019certified, fawzi2018adversarial, mahloujifar2019curse, shafahi2018adversarial, gilmer2018adversarial, madry2017towards}.

\paragraph{Theoretical constructions which incidentally exploit non-robust features.} A complimenting hypothesis is that because models trained to maximize accuracy will naturally utilize non-robust data, regardless of whether it aligns with human perception~\cite{ilyas2019adversarial} they add a low-magnitude weight to sensitive variables that can get overamplified by adversarial examples~\cite{bubeck2019adversarial, nakkiran2019adversarial}. The assumption is that this happens due to computational constraints or model complexity.

\paragraph{Insufficient data.} Schmidt et al. argue~\cite{schmidt2018adversarially} that adversarial vulnerabilities are intrinsic to statistical learning in high-dimensional spaces and not merely due to flaws in specific algorithms or architectures. This is a natural consequence of the mental model proposed by Ilyas et al.\ \cite{ilyas2019adversarial}. They also argue that due to information loss in a robust dataset, significantly more data is required during training in order to achieve comparable performance.

\paragraph{Boundary Tilting.} A competing view by Tanay and Kim et al.\ \cite{tanay2016boundary,kim2019bridging} suggests that adversarial examples exist because decision boundaries extend beyond the actual data manifold and can lie uncomfortably close to it, essentially viewing adversarial examples as a consequence of overfitting. This observation can be quantified through the concept of adversarial strength, which relates to the angular deviation between the classifier and the nearest centroid classifier. The authors also argue that this vulnerability can be addressed through proper regularization techniques.

\paragraph{Test Error in Noise.} There might be a link between robustness to random noise and adversarial attacks~\cite{fawzi2016robustness, lecuyer2019certified, cohen2019certified, ford2019adversarial}. This might imply that adversarial examples exploit inherent weaknesses in how models generalize under noisy or perturbed conditions.

\paragraph{Local Linearity.} Goodfellow, Shlens and Szegedy et al.\ \cite{goodfellow2014explaining, madry2017towards} argue that even though DNNs are highly nonlinear overall, their behavior in high-dimensional spaces often resembles that of linear models. This makes the models vulnerable to small, targeted perturbations similar to how they are computed by FGSM. However some adversarial examples are successful all while defying the assumption of local linearity and reducing a model's linearity does not necessarily improve its robustness either~\cite{athalye2018obfuscated}.

\paragraph{Piecewise-linear decision boundaries.} In the ``dimpled manifold hypothesis''~\cite{shamir2021dimpled} the central claim is that adversarial examples emerge because we attempt to fit high $n-1$ dimensional decision boundaries to inherently low-dimensional data like images (which can be losslessly projected to $k \ll n$ dimensions). This leaves redundant dimensions on which adversarial examples will not be judged, which enables them exist roughly perpendicularly from the true location of the low-dimensional natural image, by using large gradients. In this mental model adversarial examples can be on-manifold or off-manifold, based on the angle of the gradients relative to the data manifold.

The authors also suggest that decision boundaries of neural networks evolve during training. This happens through two distinct phases. First, there is a rapid ``clinging'' phase where the decision boundary moves close to the low-dimensional image manifold containing the training examples. This is followed by a slower ``dimpling'' phase that creates shallow bulges in the decision boundary, pushing it to the correct side of the training examples, without shifting the plane. This gradient-descent-based process is highly efficient, but it also leaves a brittle decision boundary that can be easily exploited.

This implies that any attempt to robustify a network by limiting all its directional derivatives will make it harder to train and thus less accurate.

It also explains why networks trained on incorrectly labeled adversarial examples can still perform well on regular test images, as the main effect of adversarial training is simply to deepen these dimples in the decision boundary.

Lukas Karner successfully was able to successfully reproduce the experiments from the ``Dimpled Manifold Hypothesis'' paper in 2023~\cite{karner2023dimpled}. He additionally demonstrated that dimensionality reduction increases the interpretability of the perturbations to humans~\cite{karner2023dimpled}.

However, despite the experiments being carried out correctly themselves, the chain of reasoning might be flawed, as shown by a succinct (<100 LoC) counterexample by Yannik Kilcher in 2021~\cite{kilcher2021dimpledcode, kilcher2021dimpled}. While the ``Dimpled Manifold Hypothesis'' implies a relatively uniform vulnerability across all dimensions the counter experiment contradicts these assumptions through successful adversarial attacks constructed by perturbing either an arbitrary subset of selected dimensions or their complement. If the decision boundary truly ``clung'' to the data manifold, restricting perturbations to a subset of dimensions would not have produced successful adversarial examples. The ability to generate adversarial examples in complementary subspaces suggests the decision boundary structure is more complex than just simple dimples.

To summarize, there is no consensus on the root cause of adversarial examples and the field remains an active area of research. The mental models proposed by different researchers are not necessarily mutually exclusive and it is likely that the true explanation involves a combination of these factors.

\section{Defenses}

Having discussed the various theories and approaches in explaining adversarial examples, we can now turn our attention to the countermeasures that have been proposed to mitigate their impact.

\subsection{Train- and Test-time defenses}

A leaderboard of adversarial robustness can be found on the RobustBench platform~\cite{croce2021robustbench}, which provides a standardized evaluation of adversarial robustness across a wide range of models and datasets. The platform includes a variety of metrics for evaluating robustness, such as the $\ell_\infty$ and $\ell_2$ adversarial perturbation sizes, as well as the robust accuracy under different attack settings.

Several effective strategies have been developed. This collection is by no means exhaustive.

\paragraph{Adversarial Training.} One of the least invasive methods to improve adversarial robustness is adversarial training. Incorporating adversarial examples into the training process improves model resilience by learning from potential attack patterns and helps maintain performance on clean data~\cite{araujo2020advocating, Ren2022VulnerabilityAR}. However, this requires the anticipated attacks to be known in advance. An alternative would be introducing derived variables for controlled randomness to input data during training, which is still effective~\cite{Adeke2023SecuringNT}.

\paragraph{Quality Assessment Integration.} Implementing image quality assessment combined with knowledge distillation helps detect potentially harmful inputs that could cause incorrect model predictions~\cite{feng2020towards}. Another alternative preprocessing technique is using brain-inspired encoders~\cite{Rakhimberdina2022StrengtheningRU}. This method is particularly effective as it does not require model retraining, but depending on the preprocessing technique used, it can be computationally expensive.

\paragraph{Moving Target Defense.} Using heterogeneous models, diversifying the model structure, using ensembles and dynamic model switching can protect against white-box adversarial attacks. This approach will make attack vectors that work on one model ineffective on others~\cite{Li2023wAdvMTDAM}.

\paragraph{Statistical Detection.} Statistical tests can be employed for some signal-based deep learning systems to detect adversarial examples. This includes analyzing a peak-to-average-power ratio and examining softmax outputs of the model~\cite{KokaljFilipovic2019AdversarialEI}.

\paragraph{Enhanced Transformation.} Transformation-based defense strategies, such as using generative adversarial networks (GANs), can help recover from adversarial examples. These methods can counteract adversarial effects while maintaining or even improving classification performance~\cite{Zhao2023EITGANAT}.

The countermeasures discussed so far provide a diverse array of techniques to mitigate the impact of adversarial examples. Each method addresses specific aspects of the problem, ranging from input preprocessing to model architecture adjustments and training methodologies. Notably, hybrid strategies that combine multiple techniques often yield the best results, with some implementations achieving reliable performance even under sophisticated attack benchmarks~\cite{ji2023benchmarking}.

\subsection{Architectural Defenses}

Assuming that robustness and generalizability are not competing objectives but complementary goals, the ultimate defense lies in designing architectures that inherently integrate robustness and interpretability~\cite{rauker2023toward}. By prioritizing these objectives at the core of model development, we can create systems that not only withstand adversarial attacks but also offer more trustworthy and transparent decision-making.

\paragraph{Fermi-Bose Machine.} One noteworthy example is the Fermi-Bose Machine~\cite{xie2024fermi}. Unlike traditional neural networks that rely on backpropagation, this method introduces a local contrastive learning mechanism inspired by quantum mechanics principles. The system works by making representations of inputs with identical labels cluster together (like bosons), while representations of different labels repel each other (like fermions). This layer-wise learning approach is considered more biologically plausible than traditional backpropagation~\cite{xie2024fermi}. The researchers demonstrated the effectiveness of their method on the MNIST dataset, showing that by adjusting the target fermion-pair-distance parameter, they could significantly reduce the susceptibility to adversarial attacks that typically disturb standard perceptrons~\cite{xie2024fermi}. The key innovation lies in controlling the geometric separation of prototype manifolds through the target distance parameter, as revealed by statistical mechanics analysis~\cite{xie2024fermi}.

\paragraph{Ensemble everything everywhere.} A recent (August 2024) state-of-the-art approach works by multi-resolution input representations and dynamic self-ensembling of intermediate layer predictions~\cite{fort2024ensemble}. The researchers introduced a robust aggregation mechanism called CrossMax, based on Vickrey auction, which combines predictions from different layers of the network~\cite{fort2024ensemble}.

The method achieved impressive results without requiring adversarial training or additional data, reaching approximately 72\% adversarial accuracy on CIFAR-10 and 48\% on CIFAR-100 using the RobustBench AutoAttack suite~\cite{fort2024ensemble}. When combined with simple adversarial training, the performance improved further to 78\% on CIFAR-10 and 51\% on CIFAR-100, surpassing the current state-of-the-art by 5\% and 9\% respectively~\cite{fort2024ensemble}.

An interesting secondary outcome of this research was the discovery that gradient-based attacks against their model produced human-interpretable images of target classes~\cite{fort2024ensemble}. Additionally, the multi-resolution approach enabled the researchers to transform pre-trained classifiers and CLIP models into controllable image generators, while also developing successful transferable attacks on large vision language models~\cite{fort2024ensemble}.

\section{Future Directions}

Perhaps we should be rethinking unrestricted adversarial examples not as attacks but as indicators of insufficient generalization, which cannot always be measured by accuracy on a predefined test set alone.

The most promising path forward may not lie in defending against these examples, but rather in fundamentally reimagining model architectures with reliability, robustness and interpretability as core design principles. This way, robustness becomes a natural byproduct of the model's structure, at no additional cost.

This perspective suggests that enhancing adversarial robustness requires developing new architectures from the ground up that inherently exhibit these properties, rather than patching existing systems.

% 
% experiments 1
% 

\chapter{Experiments: HCaptcha Inspired Geometric Masks}

When studying adversarial examples in computer vision, we are essentially dealing with a black box. Top-performing models are all deep neural networks, which are notoriously hard to interpret. This lack of interpretability means we cannot easily pinpoint why these models make certain decisions or more importantly, why they fail. This presents a challenge when trying to understand and mitigate adversarial vulnerabilities.

To address this challenge, we've adopted two guiding principles: (1) keeping it simple by using basic geometric shapes to overlay on images and (2) leveraging intermediary layers of neural networks, all to enhance interpretability.

Each principle will be discussed in a separate chapter.

\section{Research Motivation}

We noticed a gap in research when it comes to unrestricted adversarial examples that exploit the human-machine vision gap. One practical application for these types of attacks is in developing robust CAPTCHAs\footnote{Completely Automated Public Turing test to tell Computers and Humans Apart}, which are widely used to differentiate between humans and bots. The aim is to create images that machines struggle or ideally fail to recognize, while remaining easily solvable by humans, to prevent denial-of-service attacks, spam, open-source intelligence scraping and other malicious activities.

Leveraging CAPTCHAs for adversarial research offers several key benefits: (1) Their real-world effectiveness is well-established, as demonstrated by widespread adoption from major providers like Google's reCAPTCHA and hCaptcha. Studying these systems provides a robust baseline with proven resilience, allowing researchers to build on existing strengths rather than starting from scratch. (2) CAPTCHAs present a well-defined challenge with clear success criteria, facilitating future studies on their transferability to human users. Moreover, this research can be conducted with fewer ethical, legal, and financial constraints compared to many other experimental setups.

\begin{figure}
	\centering
	\includegraphics[width=1\columnwidth]{figures/ctf.png}
	\caption{ReCAPTCHAv2 cybersecurity emulation framework \\ \url{https://github.com/ETH-DISCO/captcha-the-flag}}
	\label{fig:ctf}
\end{figure}

A demo, shown in Figure~\ref{fig:ctf}, was built to showcase the potential of adversarial examples in CAPTCHAs and provide an emulation framework of reCAPTCHAv2 for penetration testing purposes. It is composed of two containers responsible for the challenger and the solver, respectively.

The first step in this direction was studying the effectiveness of CAPTCHA solvers for each provider, in the order of their market share.

Despite Google's reCAPTCHA having a global market share of at least 99\%~\cite{authkong2024recaptcha}, they have been shown to be solvable with 100\% accuracy using publicly available computer vision models on consumer hardware, by Plesner et al.~\cite{plesner2024breaking}.

HCaptchas, on the other hand, have remained undefeated in the ongoing attack-defense arms race. In fact, several dedicated open-source communities collaborating on building a solver for hCaptcha report low success rates~\cite{hcaptcha2024base64, hcaptcha_challenger}. This is coupled with our observation on sophisticated defenses being rolled out by hCaptcha on an almost weekly basis. Within the last 6 months we were studying hCaptcha, we observed the obfuscation of metadata and payloads, the introduction of new reasoning-based challenges through question answering and the introduction of new classification and segmentation challenges. This leads us to believe that hCaptcha is the most robust CAPTCHA provider on the market today.

Two hCaptcha challenges were selected for our experiments: a classification challenge and a segmentation challenge. The classification challenge overlays images with simple, predictable patterns like grids of colored geometric shapes (e.g., circles or squares). The segmentation challenge works by embedding images within a Perlin-noise-like pattern and overlaying various unrestricted perturbations.

In this scenario, the solver / identity provider acts as the adversary, while the CAPTCHA provider is the challenger. The adversary aims to bypass the CAPTCHA, and the challenger's goal is to prevent this. The adversary's success is gauged by the solver's accuracy, whereas the challenger's success is measured by the CAPTCHA's effectiveness. 

However, in traditional settings like automated content moderation on social media, the adversary aims to bypass the system by posting harmful content. Discovering simple geometric masks as robust black-box adversarial examples that transfer well between moderation systems would enable mass scale evasion at a fraction of the cost of traditional adversarial examples.

\begin{highlightbox}
	\textbf{Idea:} Evaluation on Synthetic Data \\

	One interesting idea we explored but ultimately set aside was the use of synthetic images for adversarial training and evaluation of (natural unrestricted) adversarial examples. Although promising, it did not quite align with our primary goals. We developed a pipeline that chains together several advanced models: starting with stable diffusion to generate an image, then using GPT-2 to caption it. These captions were used as text queries for zero-shot classification and detection models like CLIP and ViT, which perform exceptionally well but need a prompt to work. Finally, we used SAM1 for segmentation. This process is illustrated in Figure~\ref{fig:chained}. This is an exciting direction, especially with the advancements brought by FLUX.1~\cite{BlackForestLabs2024FLUX}, not sufficiently explored in the current literature, based on our preliminary review. \\

\begin{minted}[fontsize=\footnotesize]{python}
img = gen_stable_diffusion("an astronaut on mars riding a horse")
query = caption_gpt2(img)
probs = classify_clip(img, query)
boxes, scores, labels = detect_vit(img, query, 0.1)
masks = segment_sam1(img, boxes)
\end{minted}
\end{highlightbox}

\begin{figure}
	\centering
	\includegraphics[width=0.3\columnwidth]{figures/chained.png}
	\caption{Evaluation on synthetic data.}
	\label{fig:chained}
\end{figure}

\section{Experimental Setup}

The first series of experiments focused on evaluating the performance of state-of-the-art computer vision models on hCaptcha challenges. The goal was to assess the performance of solvers and identify potential vulnerabilities that could be exploited to generate CAPTCHA based adversarial examples. We targeted a single type of challenge: The classification challenge, which involves overlaying images with simple, predictable patterns like grids of colored geometric shapes (e.g., circles or squares).

We also studied and reconstructed the segmentation challenge, which embeds images within a Perlin-noise-like pattern, smooths the edges, adds a slight blur and sometimes incorporates the geometric masks from the classification challenge on either individual segments or the entire image. However, given the number of variables involved, we decided to focus on the classification challenge for our initial experiments and leave the other tasks for future work. Our goal was to establish a reliable baseline, not to exhaustively explore all possibilities. However, we noticed through small-scale experiments using SAM1 and SAM2 that it was significantly more difficult to solve than the classification challenge.

\paragraph{Mask Generation.}

We generated adversarial examples using 4 geometric shapes.

We developed a sublibrary that allows for the parametrizable reconstruction of geometric masks used in the classification task using the rendering engine \texttt{pycairo}. We created four distinct masks to overlay on images at varying intensities: ``Circle'', ``Diamond'', ``Square'' and ``Knit'' (the ``Word'' mask was also reconstructed, but omitted as they have been proven to be easy to mitigate~\cite{zhang2023text,dong2023robust,shayegani2023plug}).

These masks were chosen based on an experiment where we hand-labeled 1600 images from hCaptcha\footnote{Credits to Turlan Kuzhagaliyev.}. The opacities (alpha values) and densities of these masks were determined through an initial hyperparameter search, which we'll discuss later. Figure~\ref{fig:hcaptchacombined} showcases the optimized reconstructions we used to benchmark the models.

For the segmentation task, we experimented with various background textures, including Perlin noise and color-encoded multivariate Gaussian distributions. Figure~\ref{fig:segchallenge} shows both a selected example and its reconstruction. But as mentioned earlier, we decided to focus on the classification challenge for our initial experiments.

\begin{figure}[th]
	\begin{subfigure}[t]{0.19\textwidth}\centering\includegraphics[width=\linewidth]{figures/hcaptcha-circle.png}\caption{``Circle'' mask}\label{fig:subfig1}\end{subfigure}
	\hfill
	\begin{subfigure}[t]{0.19\textwidth}\centering\includegraphics[width=\linewidth]{figures/hcaptcha-diamond.png}\caption{``Diamond'' mask}\label{fig:subfig2}\end{subfigure}
	\hfill
	\begin{subfigure}[t]{0.19\textwidth}\centering\includegraphics[width=\linewidth]{figures/hcaptcha-square.png}\caption{``Square'' mask}\label{fig:subfig3}\end{subfigure}
	\hfill
	\begin{subfigure}[t]{0.19\textwidth}\centering\includegraphics[width=\linewidth]{figures/hcaptcha-word.png}\caption{``Word'' mask}\label{fig:subfig4}\end{subfigure}
	\hfill
	\begin{subfigure}[t]{0.19\textwidth}\centering\phantom{\includegraphics[width=\linewidth]{figures/hcaptcha-word.png}}\end{subfigure}
	
	\begin{subfigure}[t]{0.19\textwidth}\centering\includegraphics[width=\linewidth]{figures/hcaptcha-copy-circle.png}\caption{``Circle'' mask\\(reconstructed)}\label{fig:subfig5}\end{subfigure}
	\hfill
	\begin{subfigure}[t]{0.19\textwidth}\centering\includegraphics[width=\linewidth]{figures/hcaptcha-copy-diamond.png}\caption{``Diamond'' mask\\(reconstructed)}\label{fig:subfig6}\end{subfigure}
	\hfill
	\begin{subfigure}[t]{0.19\textwidth}\centering\includegraphics[width=\linewidth]{figures/hcaptcha-copy-square.png}\caption{``Square'' mask\\(reconstructed)}\label{fig:subfig7}\end{subfigure}
	\hfill
	\begin{subfigure}[t]{0.19\textwidth}\centering\includegraphics[width=\linewidth]{figures/hcaptcha-copy-knit.png}\caption{``Knit'' mask\\(custom)}\label{fig:subfig8}\end{subfigure}
	\hfill
	\begin{subfigure}[t]{0.19\textwidth}\centering\includegraphics[width=\linewidth]{figures/hcaptcha-copy-original.png}\caption{Original image\\(for reconstructions)}\label{fig:subfig9}\end{subfigure}

	\caption{Selected examples by hCAPTCHA and their optimized reconstructions. The ``Word'' overlay was omited and replaced with a custom ``Knit'' mask.}
	\label{fig:hcaptchacombined}
\end{figure}

\begin{figure}[th]
	\centering
	\begin{minipage}[t]{0.49\textwidth}\centering\includegraphics[width=\textwidth]{figures/hcaptcha-seg.png}\caption{Segmentation Challenge.}\label{fig:segchallenge}\end{minipage}
	\hfill
	\begin{minipage}[t]{0.49\textwidth}\centering\includegraphics[width=\textwidth]{figures/hcaptcha-seg-baiba.png}\caption{Reconstruction (arbitrary parameters).}\label{fig:segreconstruction}\end{minipage}
\end{figure}

\paragraph{Model Selection.}

We benchmarked 5 SOTA models, that can run on consumer hardware.

To identify the best zero-shot open vocabulary classification, object detection and segmentation models that can run on consumer hardware, we did a near exhaustive literature review. We looked at focused research papers~\cite{wang2024benchmarking, goldblum2024battle} and checked out public leaderboards from HuggingFace, TIMM, PapersWithCode, GitHub Trends, PyTorch benchmarks and more. Our goal was to ensure that the solver could break CAPTCHAs on any machine with minimal setup.

After the literature review we clustered the models by their architecture family and clustered them on a scatter plot based on their performance, inspired by~\cite{howard_image_models}. We then selected the top models from each family and attempted to run them on a consumer machine, assuming that they are publicly available. This was to test the feasibility of running these models in a real-world scenario.

We chose several models to test, including ``ConvNeXt\_XXLarge''~\cite{Liu_2022_CVPR}, Open CLIP's ``EVA01-g-14-plus''~\cite{Fang_2023_CVPR} and ``EVA02-L-14''~\cite{fang2024eva}, the original ``ViT-H-14-378-quickgelu''~\cite{dosovitskiy2021imageworth16x16words} and ``ResNet50x64''~\cite{He_2015_ICCV}. We highlight results for a subset of these models, specifically ConvNeXt, EVA01, EVA02, ViT-H-14 and ResNet50. These models were picked to cover key architectures in both convolutional and transformer-based approaches, allowing us to see how well our masks work across different architecture families. Additionally, we also evaluated a custom adversarially trained ResNet-50, but due to its poor performance, we decided to exclude it from the final results.

When specific models are not mentioned in tables, experiments or figures, we are implicitly referring to the average performance of all the models listed.

\paragraph{Dataset Selection.}

We evaluated against an enriched ImageNet dataset.

For experiments we utilized the enriched ImageNet dataset provided by ``visual-layer'' on HuggingFace. This dataset includes 1,000 distinct classes, offering a comprehensive range of categories for our experiments.

\paragraph{Evaluation Metrics.}

We measured the drop in accuracy and perceptual quality using a proxy metric.

Perceptual quality is about how visually similar the adversarial examples are to the original images. We used a weighted average metric to get a comprehensive view of image quality. This metric combines cosine similarity (15\% weight)~\cite{singhal2001modern}, Peak Signal-to-Noise Ratio (PSNR, 25\% weight)~\cite{9311108}, Structural Similarity Index (SSIM, 35\% weight)~\cite{wang2004image} and Learned Perceptual Image Patch Similarity (LPIPS, 25\% weight)~\cite{lpips}. These weights were chosen to balance the importance of each component in assessing overall image quality. We decided to omit the Fréchet Inception Distance (FID), due to its high computational cost. Overall, we want this number to be as high as possible.

$$
\text{Perceptual Quality} = 0.15 \times \text{Cosine Similarity} + 0.25 \times \text{PSNR} + 0.35 \times \text{SSIM} + 0.25 \times \text{LPIPS}
$$

On the accuracy front, we looked at how well the models performed in predicting the correct class. The models output a list of classes sorted by likelihood. We measured accuracy using the top-k accuracy metric, specifically accuracy@1 (Acc@1) and accuracy@5 (Acc@5). This tells us how often the correct label is in the top 1 or top 5 predicted classes, respectively. Finally, we calculated the adversarial accuracy (AdvAcc) as the difference in accuracy between adversarial and benign examples. This metric gives us a sense of how much the performance decreases relative to the original accuracy. We want this number to be as low as possible.

$$
\text{Adversarial Accuracy} = \frac{(\text{Acc@1}_{\text{adv}} + \text{Acc@5}_{\text{adv}}) / 2 - (\text{Acc@1} + \text{Acc@5}) / 2 + 1}{2}
$$

This brings us to our final evaluation metric, the final score (Score). It is calculated as the difference between perceptual quality and adversarial accuracy. Ideally, we want this number to be as high as possible.

$$
\text{Final Score} = \text{Perceptual Quality} - \text{Adversarial Accuracy}
$$

\paragraph{Hyperparameter Search.}

A grid search attempted to find the best mask parameters.

Having determined our evaluation metrics, dataset and models, we moved on to the hyperparameter search to find the optimal adversarial masks to benchmark with. We parameterized three variables: ``opacity'' (alpha value of the overlay), ``density'' (shapes per row/column and nesting, ranging from 0-100) and ``epsilon'' (for white-box FGSM attacks with CLIP-ViT on ImageNet). Using a grid search, we aimed to find the optimal values for these parameters.

We ran a hyperparameter grid search using the ``visual-layer/imagenet-1k-vl-enriched'' dataset on HuggingFace, testing 5-20 examples per combination on the validation set. For this phase, we exclusively used the CLIP ViT model due to its strong adversarial robustness, as highlighted by~\cite{wang2024roz}.

Our optimization metric combined the difference in model accuracy before and after applying the mask with an average of three perceptual quality metrics. To find the best parameters, we selected examples with the highest perceptual quality for each level of accuracy difference and performed a linear regression. We then focused on samples above the regression line in multidimensional space. This approach was more manageable than our attempts with multi-objective optimization involving multiple variables.

Our initial grid search revealed some interesting patterns. We found that achieving high perceptual quality generally required a combination of low opacity and high density. Specifically, the optimal opacity range appeared to be between 50 and 200. Additionally, higher density values consistently yielded better results. Among the various masks we tested, the diamond mask stood out as offering the best balance between effectiveness and perceptual quality. Based on these findings, the diamond mask seemed most promising for the following steps of our experiments. Figure \ref{fig:tradeoff-1} shows the trade-off between accuracy and perceptual quality for the strongest masks, where masks are visually encoded through shapes, opacities through alpha values and densities through the size of the shapes. A linear regression line is shown to highlight the areas of interest.

Our second grid search, which combined FGSM perturbations with the chosen masks, revealed some interesting insights. We found that combining perturbations with the masks generally led to worse results. The best results were obtained using the diamond mask with FGSM disabled, a density of 50 and opacities of 150 or 170. This further validated our previous findings and hinted that FGSM should be used with caution when combined with masks. Figure \ref{fig:tradeoff-2} shows the trade-off between accuracy and perceptual quality for the diamond mask combined with different opacities, densities and perturbation settings.

While these results are not representative of the entire dataset or model space, they provide a good starting point for our experiments. We can conclude not to use perturbations with the masks. These initial findings also helped us identify the best configurations in opacity and density, to be used in the following experiments. The final configurations of each mask are shown in figure \ref{fig:hcaptchacombined}.

\begin{figure}
	\centering
	\includegraphics[width=1\columnwidth]{figures/eval_cls_mask_density.pdf}
	\caption{Accuracy-perceptibility trade-off: We compare the strength of all our masks with different opacities and densities against their perceptibility based on our proxy metric.}
	\label{fig:tradeoff-1}
\end{figure}

\begin{figure}
	\centering
	\includegraphics[width=1\columnwidth]{figures/eval_cls_perturb.pdf}
	\caption{Accuracy-perceptibility trade-off: We compare the strength of a diamond mask with different opacities, densities and FGSM perturbation settings against their perceptibility based on our proxy metric.}
	\label{fig:tradeoff-2}
\end{figure}

\section{Results}

\paragraph{Attack Transferability.}

We evaluated the individual masks for their transferability between different models by averaging the ``final score'' across all models. This score serves as a proxy for the trade-off between adversarial effectiveness and perceptual quality. We found that opacity was the most significant parameter, regardless of mask specifics. Therefore, we chose the best-performing density parameter from the hyperparameter search and applied different masks and opacities to the models.

The results of our experiment are visualized in Figure~\ref{fig:accpercept} and summarized in Table~\ref{tab:transferability}. The figure shows a clear trend in the trade-off between adversarial effectiveness and perceptibility. The plot highlights an inverse relationship between these two factors, as indicated by the polynomial regression curve of degree 1. This relationship suggests that as the effectiveness of the adversarial attack increases (lower $\Delta$ Accuracy Rank), the perceptual quality of adversarial examples tends to decrease. While this is somewhat expected, we do see instances where there's a significant drop in rank (>10) while maintaining relatively high perceptual quality (>0.4).

The different mask types (circle, square, diamond and knit) and opacity levels show a range of performance across this trade-off spectrum. The scatter plot highlights clusters of points for each mask type, with some masks consistently striking a better balance between attack effectiveness and perceptual quality. Most importantly, these geometric pattern masks transfer well between state-of-the-art models.

The circle masks, contrary to our initial hyperparameter search, seem the most effective at reducing accuracy across all models and opacities. The knit mask, on the other hand, has a much smaller impact on accuracy.

These results demonstrate that using geometric masks is a practical method for generating adversarial examples with minimal computational demands for benchmarking in future experiments.

\begin{figure}
	\centering
	\includegraphics[width=1\columnwidth]{figures/eval_cls_generalizability.pdf}
	\caption{Accuracy vs. Perceptual Quality Trade-off}
	\label{fig:accpercept}
\end{figure}

\begin{table}[h]
    \centering
    \begin{tabular}{rlrrr}
        \toprule
        Opacity & Mask & $\Delta$ Acc Rank & Quality & Score \\
        \midrule
        \multirow{4}{*}{50}
            & Circle & -14.57 & 0.45 & 15.02 \\
            & Diamond & -3.27 & 0.50 & 3.76 \\
            & Knit & -0.66 & 0.54 & 1.19 \\
            & Square & -5.04 & 0.49 & 5.54 \\
        \midrule
        \multirow{4}{*}{80}
            & Circle & -52.72 & 0.31 & 53.03 \\
            & Diamond & -13.72 & 0.36 & 14.08 \\
            & Knit & -2.03 & 0.41 & 2.44 \\
            & Square & -22.01 & 0.37 & 22.37 \\
        \midrule
        \multirow{4}{*}{110}
            & Circle & -113.07 & 0.21 & 113.27 \\
            & Diamond & -39.55 & 0.26 & 39.81 \\
            & Knit & -3.62 & 0.32 & 3.93 \\
            & Square & -60.57 & 0.27 & 60.84 \\
        \midrule
        \multirow{4}{*}{140}
            & Circle & -203.89 & 0.12 & 204.01 \\
            & Diamond & -90.79 & 0.18 & 90.97 \\
            & Knit & -5.47 & 0.24 & 5.71 \\
            & Square & -134.75 & 0.18 & 134.94 \\
        \midrule
        \multirow{4}{*}{170}
            & Circle & -310.80 & 0.07 & 310.88 \\
            & Diamond & -188.92 & 0.12 & 189.04 \\
            & Knit & -9.21 & 0.18 & 9.39 \\
            & Square & -264.90 & 0.12 & 265.02 \\
        \bottomrule
    \end{tabular}
    \vspace*{0.1cm}
    \caption{Transferability of Masks}
    \label{tab:transferability}
\end{table}
	
\paragraph{Model Robustness.}

We inspected individual models, examining both the drop in acc@1 and acc@5. This will give us a more detailed understanding of how the masks impact the individual models. The results are summarized in Table~\ref{tab:drop-acc1} and Table~\ref{tab:drop-acc5} for top-1 and top-5 accuracy, respectively.

A negative value indicates an improvement in accuracy, while a positive value indicates a decrease. Larger values indicate a stronger effect of the adversarial examples.

While the previous perspective on the data compared masks against each other, this view allows us to compare the models against each other. We can see that while all models show very similar overall trends in their behavior, the ResNet model seems particularly sensitive to the Diamond and Knit pattern in contrast to the others. This is particularly interesting, given that the Knit pattern is the least effective mask overall, as seen in the previous step.

\begin{table}[h]
	% this was accidentally mislabeled as subset-500 by andreas in the paper - it's actually the full imagenet dataset
	% see: https://github.com/ETH-DISCO/advx-bench/blob/main/analysis/4-eval_cls_modelperf.ipynb

    \setlength{\tabcolsep}{2pt}
    % \tiny

    \centering
    \begin{minipage}{0.48\textwidth}
        \centering

        \begin{tabular}{ll|rrrrr}
			\multicolumn{2}{c|}{} & \multicolumn{5}{c}{Opacity} \\
			Model & Mask &  19\% & 31\% & 43\% & 54\% & 66\% \\
			\midrule
			\multirow{4}{*}{ConvNeXt} & Circle & 13.0  & 33.6  & 51.2  & 64.6  & 69.2  \\
			& Diamond & 4.8  & 13.6  & 31.8  & 49.6  & 64.6  \\
			& Knit & 2.2  & 3.2  & 8.0  & 11.4  & 18.0  \\
			& Square & 6.8  & 18.4  & 36.4  & 52.0  & 65.6 \\\midrule
			\multirow{4}{*}{EVA01} & Circle & 7.2  & 15.4  & 33.0  & 49.2  & 65.0  \\
			& Diamond & 2.6  & 8.6  & 19.6  & 33.0  & 54.8  \\
			& Knit & 1.2  & 1.2  & 4.4  & 6.6  & 10.6  \\
			& Square & 4.2  & 9.0  & 17.4  & 31.4  & 55.8 \\\midrule
			\multirow{4}{*}{EVA02} & Circle & 9.4  & 19.0  & 31.4  & 50.4  & 63.8  \\
			& Diamond & 2.4  & 5.6  & 10.6  & 19.0  & 38.0  \\
			& Knit & 2.8  & 4.8  & 5.2  & 6.8  & 8.8  \\
			& Square & 6.8  & 12.4  & 20.8  & 37.4  & 61.8 \\\midrule
			\multirow{4}{*}{ResNet} & Circle & 31.0  & 54.6  & 60.0  & 62.4  & 63.4  \\
			& Diamond & 13.2  & 31.6  & 50.4  & 59.4  & 62.2  \\
			& Knit & 5.0  & 11.2  & 14.4  & 19.4  & 27.6  \\
			& Square & 15.2  & 38.8  & 56.0  & 62.2  & 63.4 \\\midrule
			\multirow{4}{*}{ViT-H-14} & Circle & 5.8  & 20.6  & 48.2  & 70.8  & 80.2  \\
			& Diamond & 2.0  & 5.4  & 15.2  & 34.4  & 61.8  \\
			& Knit & 1.6 & 2.4  & 2.8  & 6.2  & 8.0  \\
			& Square & 3.2  & 9.6  & 25.0  & 54.2  & 77.2  \\
        \end{tabular}
        \vspace*{0.1cm}
        \caption{Change of Acc@1 in [\%].}
        \label{tab:drop-acc1}
        
    \end{minipage}%
    \hfill
    \begin{minipage}{0.48\textwidth}
        \centering

        \begin{tabular}{ll|rrrrr}
            \multicolumn{2}{c|}{} & \multicolumn{5}{c}{Opacity} \\
            Model & Mask &  19\% & 31\% & 43\% & 54\% & 66\% \\
            \midrule
            \multirow{4}{*}{ConvNeXt} & Circle & 7.60 & 29.60 & 54.80 & 73.40 & 85.00 \\
             & Diamond & 2.60 & 8.80 & 24.20 & 51.40 & 71.60 \\
             & Knit & 1.80 & 2.20 & 4.60 & 7.80 & 13.20 \\
             & Square & 4.80 & 13.20 & 28.80 & 54.80 & 76.80 \\\midrule
            \multirow{4}{*}{EVA01} & Circle & 4.80 & 14.00 & 27.80 & 50.60 & 75.40 \\
             & Diamond & 2.40 & 6.60 & 14.80 & 31.00 & 57.60 \\
             & Knit & 1.40 & 2.80 & 4.60 & 6.20 & 8.00 \\
             & Square & 3.40 & 7.00 & 12.60 & 28.20 & 61.00 \\\midrule
            \multirow{4}{*}{EVA02} & Circle & 4.60 & 12.20 & 24.40 & 44.60 & 65.00 \\
             & Diamond & 1.40 & 3.60 & 6.60 & 14.80 & 34.80 \\
             & Knit & 0.40 & 0.40 & 1.80 & 3.20 & 4.80 \\
             & Square & 2.20 & 6.60 & 15.00 & 31.40 & 63.60 \\\midrule
            \multirow{4}{*}{ResNet} & Circle & 34.20 & 67.40 & 80.40 & 85.40 & 86.20 \\
             & Diamond & 12.20 & 28.80 & 56.00 & 75.60 & 85.00 \\
             & Knit & 4.40 & 8.00 & 10.20 & 15.00 & 20.80 \\
             & Square & 15.20 & 40.20 & 66.40 & 82.20 & 86.60 \\\midrule
            \multirow{4}{*}{ViT-H-14} & Circle & 2.60 & 16.20 & 46.00 & 77.60 & 90.80 \\
             & Diamond & 0.20 & 2.20 & 10.60 & 28.60 & 61.20 \\
             & Knit & -0.60 & 0.60 & 1.00 & 2.00 & 3.20 \\
             & Square & 1.40 & 6.40 & 18.60 & 50.20 & 82.80 
        \end{tabular}
        \vspace*{0.1cm}
        \caption{Change of Acc@5 in [\%].}
        \label{tab:drop-acc5}

    \end{minipage}
\end{table}

\section{Conclusion}

In our first set of experiments, we tested geometric masks against state-of-the-art models. While the results yielded several insights, two key takeaways stand out as especially relevant for the rest of this work.

\paragraph{Geometric mask attacks are simple, effective and transferable.}

Our experiments show that hCAPTCHA-inspired geometric masks are (1) simple, (2) effective, noticeably reducing model accuracy, and (3) transferable, as they perform similarly across different models. The advantage they offer over traditional imperceptible black-box adversarial examples is their simplicity, which makes it feasible to reason about their influence on models, even in latent spaces. This enables us to break down the complexity of adversarial vulnerabilities and understand the underlying mechanisms that make them effective.

\paragraph{ResNet is an outlier.}

We observed that most state-of-the-art architectures exhibited similar accuracy distributions when subjected to geometric masks. However, ResNet stood out as unusually sensitive to the Diamond and Knit pattern \textendash{} a curious finding, given that this pattern is the least effective mask overall when averaged across all models. This suggests that the ResNet model results should be interpreted with caution, as they may not fully represent the behavior of other models under geometric mask attacks.

% 
% experiments 2
% 

\chapter{Experiments: Self-Ensembled ResNet}

In this chapter, we present our findings on the self-ensembled ResNet model.

\section{Research Motivation}

The self-ensembled ResNet~\cite{fort2024ensemble}, as presented in the introduction, is a novel approach to enhancing adversarial robustness. It combines multi-resolution inputs with dynamic self-ensembling of predictions from intermediate layers of the neural network. The multi-resolution input strategy involves feeding the model multiple versions of an image at different resolutions, inspired by biological mechanisms like eye saccades, which enhances robustness by forcing the network to process diverse representations simultaneously. The self-ensembling aspect leverages the inherent robustness of the intermediate layer predictions, which are less affected by adversarial attacks targeting the final classifier. These predictions are aggregated using a consensus algorithm called ``CrossMax'', inspired by Vickrey auctions, which dynamically selects consistent outputs across layers:

\begin{samepage}
\begin{spacing}{1}
\begin{minted}[fontsize=\footnotesize]{python}
def get_cross_max_consensus_logits(outputs: torch.Tensor, k: int) -> torch.Tensor:
    # subtract the max per-predictor over classes
    Z_hat = outputs - outputs.max(dim=2, keepdim=True)[0]
    # subtract the per-class max over predictors
    Z_hat = Z_hat - Z_hat.max(dim=1, keepdim=True)[0]
    # get highest k values per class
    Y, _ = torch.topk(Z_hat, k, dim=1)
    # get the k-th highest value per class
    Y = Y[:, -1, :]
    assert Y.shape == (outputs.shape[0], outputs.shape[2])
    assert len(Y.shape) == 2
    return Y
\end{minted}
\end{spacing}
\end{samepage}

Architecturally, what makes this approach interesting is its ability to use a single ResNet model to create a ``self-ensemble'' by decoupling and aggregating predictions from intermediate layers. This eliminates the need for multiple independent models while still achieving ensemble-like robustness. Additionally, the use of multi-resolution inputs and stochastic augmentations reduces the attack surface for adversarial perturbations.

To the best of our knowledge, we are the first team to reproduce, evaluate and improve upon this approach.

\section{Experimental Setup}

We conducted a series of experiments on the self-ensembled ResNet and the standard ResNet-50 as a baseline with the same weights, trained on ImageNet to evaluate the new architecture against. We build upon our findings from the previous chapter.

\paragraph{Reproduction.}

We discarded our reproduction after the authors shared their code.

We chose to recreate the architecture\footnote{Credits to Andreas Plesner.}, the CrossMax consensus algorithm and our own distributed multi-GPU training setup from the ground up, as the authors had not yet shared their codebase, when we started our experiments. This allowed us to gain a deeper understanding of the model and validate the hyperparameters used in the original paper.

We ran a set of experiments on CIFAR10 and CIFAR100 to confirm our reproduction was sensible and to find the best hyperparameters. We used the same ResNet architecture as the authors but slightly adjusted the training pipeline. Our hyperparameter search space consisted of: the dataset (CIFAR10, CIFAR100), learning rate (1e-1, 1e-4, 1e-5, 1.7e-5, 1e-6, 1e-7), number of epochs (4, 8, 16) and CrossMax $k$ (2, 3).

We found the best results with a learning rate of 1e-4, running for 16 epochs and setting CrossMax $k$ to 2. Interestingly, running more epochs only bumped up performance by less than 1\%, so we stuck with 16 epochs. For CrossMax $k$, 2 was the sweet spot for both datasets, although CIFAR10 showed a tiny edge with $k=3$ by just 0.1\%. These results are visualized in Figure~\ref{fig:se-hyperparams}. When the authors later shared parts of their implementation and hyperparameters, our findings were in line with theirs, except for slight differences in the learning rate (3.3e-5 vs 1e-4) and the number of epochs (6 vs 16). We believe these differences to be due to the computational limits they set for their experiments. Figure~\ref{fig:se-hyperparams} shows the results of our hyperparameter search on our reproduction of the self-ensembled ResNet.

After the authors shared their codebase, we transitioned to their implementation to maintain consistency and ensure our results were directly comparable.

\begin{figure}
	\centering
	\includegraphics[width=1\columnwidth]{figures/hyperparams.pdf}
	\caption{Hyperparameter search on the reproduced architecture.}
	\label{fig:se-hyperparams}
\end{figure}

\paragraph{Training Strategy Selection.}

We compared $2^3$ training configurations.

The author's code included several improvements not mentioned in the paper, such as random resolution shuffling, which were recommended in the code comments for better results.

We aimed to keep as many hyperparameters unchanged as possible while comparing the impact of three specific training enhancements: (1) nature-inspired noise, (2) channel shuffling and (3) light FGSM adversarial training, resulting in $2^3$ possible training configurations.

The nature-inspired noise works by adding noise, applying contrast adjustments, shifting pixel positions (jittering) and modifying resolutions. In the last step it combines these augmented outputs into a multichannel input to enhance training. The random channel resolution shuffling randomly shuffles the resolutions in the stack, while the light FGSM adversarial training applies FGSM adversarial training for a few epochs to improve adversarial robustness.

Initially, we experimented with each technique independently to evaluate their effects. For example, applying random shuffling alone reduced adversarial robustness, whereas combining all three consistently produced the best results. We did not observe any negative interactions between these techniques, as they all seemed to complement each other. The results of these experiments are visualized in Figure~\ref{fig:8train-cifar10} and Figure~\ref{fig:8train-cifar100}. This led us to either exclusively use all three techniques or none at all in our subsequent experiments. We simply refer to this enhanced training strategy as ``natural training'' (or just adversarial training) in the following.

Our initial evaluations of the training strategies focused on CIFAR-10 and CIFAR-100, as specified in the authors' codebase. While the original paper also utilized MNIST, we opted for Imagenette due to its relevance to higher-resolution tasks. However, the aggressive downsampling and upsampling in the code proved problematic for Imagenette, leading us to exclude it from our preliminary evaluation. These issues were addressed in our codebase for future experiments.

\paragraph{Dataset Selection.}

We evaluated on CIFAR-10 only.

Despite implementing and configuring our evaluation pipeline for 3 datasets (CIFAR-10, CIFAR-100 and Imagenette), we only evaluated CIFAR-10 due to time constraints. These constraints were due to the limited availability of GPUs with at least 80 GB memory, which is required by the self-ensembled ResNet model. We were unable to reduce the model's memory requirements, but we did identify compute bottlenecks in the codebase, some of which we also addressed as a pull request to the authors.

However, as visible in Figure~\ref{fig:8train-cifar10} vs.\ Figure~\ref{fig:8train-cifar100}, classifying CIFAR-10 is clearly less challenging than other datasets for the self-ensembled ResNet and this dataset is not representative for the others. Therefore, our results should be taken with a grain of salt and further evaluation is necessary for more conclusive results. This should however not diminish the value of our findings on CIFAR-10 and our methodological contributions.

\begin{figure}
	\centering
	\includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{figures/self_ensemble_8train_cifar10.pdf}
	\caption{Training strategy combinations on CIFAR10, with the red dotted line for the ensemble. The X-axis shows layers, the Y-axis shows accuracy, the X-grid shows train configs and the Y-grid shows different attacks.}
	\label{fig:8train-cifar10}
\end{figure}

\begin{figure}
	\centering
	\includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{figures/self_ensemble_8train_cifar100.pdf}
	\caption{Training strategy combinations on CIFAR100, with the red dotted line for the ensemble. The X-axis shows layers, the Y-axis shows accuracy, the X-grid shows train configs and the Y-grid shows different attacks.}
	\label{fig:8train-cifar100}
\end{figure}

\paragraph{Attack Selection.}

We attacked the model with FGSM, PGD and 3 geometric masks.

The authors use a general benchmarking suite and visualize their attacks against FGSM. However, after some consultation\footnote{Thanks to Nicholas Carlini for his advice.} and manual inspection, we decided to also include Projected Gradient Descent (PGD) attacks in our evaluation.

For the geometric masks, we extended our rendering pipeline from previous experiments. Building on the observation that opacity and density of a mask are generally relevant across all models and the effectiveness of the ``Knit'' mask on the vanilla ResNet architecture (which we also kept, see Figure \ref{fig:attacks-v2}), we parametrized our mask generator with the following parameters: the number of sides (3, 4, 6, 10), the shapes per row and column (2, 4, 10), the number of concentric shapes (1, 2, 3, 4) and colors (True, False). We generated masks for all combinations of these parameters for our experiments. Selected examples of these masks are shown in Figure~\ref{fig:attacks-v2}.

\begin{figure}[th]
	\begin{subfigure}[t]{0.19\textwidth}\centering\includegraphics[width=\linewidth]{figures/3_2_2_False.png}\caption{Triangle mask}\label{fig:atk1}\end{subfigure}
	\hfill
	\begin{subfigure}[t]{0.19\textwidth}\centering\includegraphics[width=\linewidth]{figures/4_4_5_True.png}\caption{Square mask}\label{fig:atk2}\end{subfigure}
	\hfill
	\begin{subfigure}[t]{0.19\textwidth}\centering\includegraphics[width=\linewidth]{figures/6_2_2_True.png}\caption{Hexagon mask}\label{fig:atk3}\end{subfigure}
	\hfill
	\begin{subfigure}[t]{0.19\textwidth}\centering\includegraphics[width=\linewidth]{figures/10_10_10_False.png}\caption{Decagon mask}\label{fig:atk4}\end{subfigure}

	\caption{Selected samples of all geometric masks used in our experiments. Masks have varying sides, count of shapes per row/column, number of concentric shapes and colors.}
	\label{fig:attacks-v2}
\end{figure}

To narrow down the 96 geometric mask attacks, we did a preliminary exploratory analysis. We focused on the stronger ensemble accuracy (with and without natural training) since the ensemble consistently outperformed the last layer in all challenging experiments. This is further discussed in the results section. Figure~\ref{fig:mask-limit} shows the visual results of this analysis. We found that having 4 shapes per column and row, combined with 2 concentric shapes, was the most effective. Surprisingly, the number of sides had little impact on the mask's effectiveness, although triangles performed best overall. In every opacity, color, and side combination we tested, using colored masks (like those used by hCAPTCHA) outperformed monochrome masks by a wide margin. The most important parameters to vary in the remaining analyses were opacity, the number of shapes per row/column and whether the model was naturally trained. This leaves us with 3 masks to evaluate on, shown in Figure~\ref{fig:attacks-v2-subset}.

\begin{figure}
	\centering
	\includegraphics[width=1\columnwidth,keepaspectratio]{figures/self_ensemble_mask_exploration_cifar10.pdf}
	\caption{Limiting mask parameters to the most effective ones for CIFAR10. The lower most plot encodes the opacity in the x-axis, the better performing ensemble accuracy in the y-axis, the number of sides in the individual subplots and whether masks are colored or not as the color.}
	\label{fig:mask-limit}
\end{figure}

\begin{figure}[th]
	\begin{subfigure}[t]{0.19\textwidth}\centering\includegraphics[width=\linewidth]{figures/3_2_2_True.png}\caption{Two shapes per row/column}\label{fig:atk5}\end{subfigure}
	\hfill
	\begin{subfigure}[t]{0.19\textwidth}\centering\includegraphics[width=\linewidth]{figures/3_4_2_True.png}\caption{Four shapes per row/column}\label{fig:atk6}\end{subfigure}
	\hfill
	\begin{subfigure}[t]{0.19\textwidth}\centering\includegraphics[width=\linewidth]{figures/3_10_2_True.png}\caption{Ten shapes per row/column}\label{fig:atk7}\end{subfigure}

	\caption{The 3 most effective geometric masks against the self-ensembled ResNet, based on preliminary analysis. All masks are colored triangles with 2 concentric shapes and a varying number of per row and column.}
	\label{fig:attacks-v2-subset}
\end{figure}

\paragraph{Backbone Evaluation.}

We attacked the backbone with the same 3 geometric masks.

The self-ensembler uses the default ResNet-50 implementation from \texttt{torchvision}, pretrained on ImageNet, as its backbone. To provide a reference point for the self-ensemble, we evaluated the backbone independently with the three masks to which the self-ensemble showed the highest sensitivity, applying the same range of opacities.

For each mask and opacity combination, we evaluated 9 variants of the backbone by varying the training conditions. Specifically, we used 3 training epochs (0, 2, 6) and 3 adversarial training fractions (0, 0.1, 0.2). Each variant was tuned and evaluated with the same opacity. The results of these experiments are presented in Figure~\ref{fig:vanilla}. We believe that these adjustments, including adversarial training and tuning, create a fair comparison. This aligns with the self-ensemble, where tuning is required and linear probes for each intermediate layer are retrained from scratch on the dataset for each experiment. 

\section{Results}

The experiments discussed in this section are illustrated in Figures~\ref{fig:fgsm-pgd}, \ref{fig:mask-effective} and \ref{fig:vanilla}.

\paragraph{Natural Training.}

Looking broadly at the self-ensembled model's robustness against various attacks, we observe a consistent trend.

Natural training was not given much attention in the original paper we reproduced, even though our experiments consistently show that it plays a key role in achieving robustness. For example, while self-ensembling of the ResNet architecture is effective, it falls short when dealing with more sophisticated attacks beyond simple FGSM. When comparing naturally trained models (marked in black in the plots) to non-naturally trained ones (marked in gray), an interesting pattern emerges: initially, the non-naturally trained model underperforms on benign datasets by around 10\%. However, as attack strength increases—whether through FGSM, PGD, or masks with opacity above 32 \textendash{} we see the naturally trained model gain a substantial advantage. This is particularly striking in PGD attacks, where the accuracy of non-naturally trained models can drop close to zero when targeting the final or intermediate layers. Meanwhile, naturally trained models experience only minor accuracy reductions under the same conditions.

That said, this does not undermine the value of self-ensembling. In fact, it's highly effective in most scenarios, with the ensemble consistently outperforming the final layer. Only a few cases deviate from this trend. Attacks specifically targeting the ensemble itself are generally ineffective and combining natural training with self-ensembling produces exceptional results. When natural training is used, the ensemble's accuracy declines much more gradually as attack strength ramps up, compared to models lacking natural training.

However, we had hoped that this new architecture alone might achieve state-of-the-art robustness without requiring any form of adversarial training \textendash{} a concept we refer to as ``zero-cost robustness''. This represents an exciting vision for the future of adversarial robustness research. Unfortunately, our findings suggest we're not there yet. While self-ensembling with ResNet is a promising step forward, it's not sufficient on its own to achieve state-of-the-art robustness without adversarial training.

\paragraph{FGSM vs.\ PGD.}

As expected, the PGD attack proves a lot stronger than FGSM when tested against the self-ensembled ResNet. Despite this, both attacks exhibit similar patterns in terms of which intermediary layers they impact and the overall shape of intermediary accuracy, as shown in Figure~\ref{fig:fgsm-pgd}. In every example, the PGD attack amplifies the accuracy drop seen from FGSM, particularly for the non-naturally trained model. While FGSM reduces accuracy to around 25\%, PGD pushes it nearly to zero. This disparity is most pronounced in the final layer, where PGD leads to a drop in accuracy exceeding 40\%.

\begin{figure}
	\centering
	\includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{figures/self_ensemble_fgsm_pgd_cifar10.pdf}
	\caption{Comparing FGSM and PGD attacks on CIFAR10. The X-axis shows intermediary layers, the Y-axis shows accuracy, the black line indicates light natural training, dotted horizontal lines show ensemble performance.}
	\label{fig:fgsm-pgd}
\end{figure}

\paragraph{Geometric Masks.}

In this set of experiments, we did not rely on perceptibility proxy metrics. Instead, we manually inspected the masks to evaluate their perceptibility. We observed that the unrestricted geometric adversarial examples used as masks were not only clearly perceptible but also caused information loss. Specifically, with an opacity of 128, which left only the contours and rough edges of shapes visible. Nonetheless we could consistently and accurately identify the underlying class of the image in every case.

Among the variables involved in geometric mask attacks \textendash{} such as the number of shape sides, number of shapes per row and column, number of concentric shapes, enabling or disabling colors and opacity \textendash{} we identified the three most effective masks through exploratory analysis. We then fixed those variables as constants and focused our investigation on two variables: the number of shapes per row and column and opacity, as they had the most significant impact.

Interestingly, while varying the number of shapes per row and column yielded only marginal differences of 1-5\% in accuracy (as shown in Figure~\ref{fig:mask-effective}), changes in opacity consistently led to noticeable drops in accuracy. Doubling the opacity always resulted in at least a 10\% drop in accuracy within our tested range. Additionally, we observed a ``flipping'' effect between naturally trained and non-naturally trained models, which consistently occurred at opacity levels between 32 and 64. This behavior stood out as the most intriguing finding in this set of experiments and aligns with our current understanding of the ``robustness-accuracy trade-off'', explained in the introduction.

\begin{figure}
	\centering
	\includegraphics[width=1\columnwidth,keepaspectratio]{figures/self_ensemble_mask_final_cifar10.pdf}
	\caption{Evaluating the most effective masks on CIFAR10. The X-axis shows intermediary layers, the Y-axis shows accuracy, the black line indicates natural training, dotted horizontal lines show ensemble performance.}
	\label{fig:mask-effective}
\end{figure}

\paragraph{Improvements over the Backbone.}

Similar to the large accuracy drop observed in the self-ensembled ResNet, especially from opacity 32, we find that simply tuning the backbone on the CIFAR10 dataset without incorporating adversarial examples also results in a noticeable accuracy decline. However, when the backbone is adversarially trained using the same masks it is later evaluated on, its accuracy significantly improves, outperforming the self-ensembled ResNet and reaching nearly 80\% accuracy. This trend is illustrated in Figure~\ref{fig:vanilla}.

The key insight here is that when adversaries gain access to the mask used for generating adversarial examples and train their model against it (along with the dataset), the effectiveness of the attack diminishes considerably. This highlights that a ``robustified'' backbone, trained with adversarial examples in its tuning set, is not directly comparable to the self-ensembled model in a fair manner.

The self-ensembling approach and naturally trained self-ensemble exhibit an additional advantage: robustness across all masks, regardless of their specific properties. In contrast, the robustified backbone shows robustness only to the masks it was specifically tuned with. This finding underscores the versatility and potential of the self-ensembled ResNet.

However, in a specific case \textendash{} using 3-sided shapes, 10 shapes per row/column, 2 concentric shapes, with colors enabled \textendash{} we observe an anomaly among the high-opacity experiments. Here, even the naturally trained ensemble underperforms compared to the tuned but non-robustified backbone. While such exceptions exist, they were observed in less than 1\% of all experiments and only under particularly strong attacks.

\begin{figure}
	\centering
	\includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{figures/baseline_cifar10.pdf}
	\caption{Evaluating the most effective masks on the Backbone. The X-axis shows the ratio of perturbed masks in the tuning set, the Y-axis shows the number of epochs the model was tuned with.}
	\label{fig:vanilla}
\end{figure}

\section{Conclusion}

Our key takeaways can be summarized in two important concepts.

\paragraph{Limited Zero-Cost Robustness.}

One of the central aspirations of this research was to assess whether self-ensembling architectures could achieve state-of-the-art adversarial robustness without incurring the computational expense of adversarial training \textendash{} a concept we refer to as ``zero-cost robustness''. While self-ensembling demonstrated clear potential, particularly in its ability to outperform non-ensembled layers in most scenarios, our results indicate that it falls short of achieving robustness on par with adversarially trained models.

Attacks such as PGD revealed critical limitations of the self-ensembled ResNet, particularly when natural training was absent. The accuracy of non-naturally trained models plummeted close to zero under stronger attacks, emphasizing the need for additional mechanisms beyond architectural improvements to achieve true zero-cost robustness. Although natural training improved the model's resilience massively (maintaining accuracy by over 50\% for the highest tested opacity), it alone could not bridge the gap entirely. This suggests that while self-ensembling remains a promising approach, the vision of achieving robust, attack-resistant models without adversarial training is not yet a reality, underscoring the importance of continued research in this direction.

\paragraph{Accuracy Flipping through Natural Training.}

One of the most intriguing discoveries in our experiments was the ``accuracy flipping'' phenomenon observed under increasing attack strength. When comparing naturally trained and non-naturally trained models, we identified a consistent pattern: non-naturally trained models initially performed better on benign datasets but suffered dramatically under attack. Naturally trained models, on the other hand, demonstrated a striking resilience, with their accuracy surpassing that of non-naturally trained models as attack strength increased.

This behavior was especially pronounced when varying the opacity of geometric masks. A critical threshold emerged between opacity levels of 32 and 64, where naturally trained models not only maintained more consistent performance but also began outperforming their non-naturally trained counterparts. This ``flipping'' of accuracy highlights the nuanced interplay between robustness and accuracy within naturally trained models.

The phenomenon aligns with the broader ``robustness-accuracy trade-off'', suggesting that natural training manipulates a model's ability to generalize across both benign and adversarial examples. Moreover, it underscores the importance of taking natural training into account as a foundational component of robustness, especially when designing models aimed at withstanding unrestricted or perceptual adversarial attacks.

\paragraph{Outlook.}

In summary, our experiments highlight both the promise and the limitations of self-ensembling as a pathway toward robust machine learning. While natural training emerges as a surprisingly effective tool, particularly in counteracting stronger attacks like PGD and high-opacity geometric masks, achieving true zero-cost robustness remains an open challenge.

Future work could explore the use of MINE~\cite{pmlr-v80-belghazi18a} and Rényi's $\alpha$-order matrix-based functional~\cite{6954500} to estimate mutual information between self-ensemble layers and track the movement of latent representations across these intermediate layers. Ideally, these finer-grained metrics, combined with simple and effective black-box attacks, could lay the groundwork for foundational research in the intersection of machine learning interpretability and adversarial robustness.

\bigskip
\bibliographystyle{IEEEtran}
\bibliography{references}

\appendix

% \chapter{Appendix}

\chapter*{Overview of Generative AI Tools Used}

In this work language models were not used to generate original content. Instead, they solely enhanced spelling, grammar and style.

The ACM guidelines~\cite{acm_authorship_2024} suggest that using generative models for editing \textendash{} such as improving grammar, clarity or engagement \textendash{} does not require disclosure, likening it to traditional tools like Grammarly\footnote{Credits to Maximilian Kleinegger for this finding.}. However, in the spirit of transparency, we disclose the specific models used and the prompt given to them for each paragraph.

The following models were used:

\begin{itemize}
	\item JetBrains Grazie (version 1.7.3)
	\item Microsoft Github Copilot Pro (version 0.23)
	\item Antrophic Claude Sonnet 3.5, wrapped by Perplexity (version 2)
	\item OpenAI GPT-4o, wrapped by Perplexity (version omni)
\end{itemize}

In almost all cases, the following prompt was used, before minor adjustments were made:

\begin{verbatim}
rewrite.
do not alter the meaning of this text.

use full sentences.
do not use enumerations, itemizations, headings

use a simple, clear, to the point style.
do not use useless technical jargon.
do not use flowery language.
prefer transitive phrasal (noun comes inbetween the verb and the preposition).
do not use standard inversions.
do not use semicolons.
do not use serial commas / oxford commas.

do not use dancing metaphores.
do not use the words: AI, significant, delve, dive, deep, uncover, discover, explore, revolutionize, tapestry, intriguing, holistic, intersection, ethical considerations, area, realm, boasts, emerge, embrace, realm, underscore, embody, nestled, vibrant, blend, ample, capture, synergy, in the realm of, paramount, employed.
\end{verbatim}

Alternatively, for quick edits, the following prompt was used:

\begin{verbatim}
rewrite. use a casual but academic tone.
\end{verbatim}

\end{document}
